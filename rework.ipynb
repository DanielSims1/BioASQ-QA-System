{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree as et\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "from utils import *\n",
    "\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pmid(docs):\n",
    "    documents = [\n",
    "        document.split(\"/\")[-1] for document in docs\n",
    "    ]\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers(answers_files):\n",
    "    all_answers = dict()\n",
    "    for a in answers_files:\n",
    "        with open(a, \"r\") as f:\n",
    "            d = json.loads(f.read())\n",
    "            if DEBUG:\n",
    "                print(f\"{len(d)} answers found in {a}\")\n",
    "            for key, value in d.items():\n",
    "                if key in all_answers.keys():\n",
    "                    print(f\"MULTIPLE ANSWERS FOR {key}\")\n",
    "                if isinstance(value,list):\n",
    "                    all_answers[key] = value[0] # get the first value which is answer not prediction for the yes/no\n",
    "                else:\n",
    "                    all_answers[key] = value\n",
    "    return all_answers\n",
    "\n",
    "def get_three_files(a_dir):\n",
    "    return [\n",
    "        a_dir + \"/factoid/predictions.json\",\n",
    "        a_dir + \"/list/predictions.json\",\n",
    "        a_dir + \"/yesno/predictions.json\",\n",
    "    ]\n",
    "\n",
    "def get_col_list(gold_df,gen_df,col):\n",
    "    gold_col = gold_df.loc[:,['id',col]].copy()\n",
    "    gen_col = gen_df.loc[:,['id',col]].copy() \n",
    "    \n",
    "    gold = gold_col.to_dict(orient='list')\n",
    "    gen = gen_col.to_dict(orient='list')\n",
    "    gen_ids = gen['id']\n",
    "    gen_vals = gen[col]\n",
    "    gold_ids = gold['id']\n",
    "    gold_vals = gold[col]\n",
    "    return gold_ids,gold_vals,gen_ids,gen_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_xml(xml_file, dir_for_qa): \n",
    "    no_answers = 0\n",
    "    # get answers\n",
    "    qa_answers = get_answers(get_three_files(dir_for_qa))\n",
    "    # get ir and qu\n",
    "    df_cols = ['id','human_concepts','documents','full_abstracts','titles','type', 'exact_answer']\n",
    "    xtree = et.parse(xml_file)\n",
    "    xroot = xtree.getroot()\n",
    "    rows = []\n",
    "    for question in xroot: \n",
    "        id = question.attrib.get(\"id\")\n",
    "        ir = question.find(\"IR\")\n",
    "        qp = question.find(\"QP\")\n",
    "        concepts = [e.text for e in qp.findall(\"Entities\")]\n",
    "        qa_type = qp.find(\"Type\").text\n",
    "        titles =  [e.find(\"Title\").text for e in ir.findall(\"Result\")]\n",
    "        abstracts =  [e.find(\"Abstract\").text for e in ir.findall(\"Result\")]\n",
    "        pmids = [e.get(\"PMID\") for e in ir.findall(\"Result\")]\n",
    "        exact_answer = qa_answers[id] if id in qa_answers else None\n",
    "        if DEBUG and not exact_answer:\n",
    "            print(f\"id [{id}] has no answer\")\n",
    "            no_answers +=1\n",
    "        rows.append({\"id\":id,\"human_concepts\":concepts,\"documents\":pmids,\"full_abstracts\":abstracts,\"titles\":titles,\"type\":qa_type,'exact_answer':exact_answer})\n",
    "    out_df = pd.DataFrame(rows, columns=df_cols)\n",
    "    if DEBUG:\n",
    "        print(f\"{GREEN}[{no_answers}/{len(out_df)}]{OFF} {WHITE}questions had answers{OFF}\")\n",
    "    return out_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_yes_no_info(df, tag):\n",
    "    print(tag)\n",
    "    print(f\" [{len(df)}] {tag} Yes/No Questions\")\n",
    "    yes_df = df[df['exact_answer'] == 'yes']\n",
    "    no_df = df[df['exact_answer'] == 'no']\n",
    "    print(f\" [{len(yes_df)}] {tag} Yes Questions\")\n",
    "    print(f\" [{len(no_df)}] {tag} No Questions\")\n",
    "\n",
    "\n",
    "\"\"\" f1 Yes\n",
    "    tp is gen 'yes' | gold 'yes'\n",
    "    fp is gen 'yes' | gold 'no'\n",
    "    fn is gen 'no' |  gold 'yes'\n",
    "\n",
    "    f1 No\n",
    "    tp is gen 'no' | gold 'no'\n",
    "    fp is gen 'no' | gold 'yes'\n",
    "    fn is gen 'yes' |  gold 'no'\n",
    "\n",
    "    IGNORE if the predicted type is yes/no but gold type is different\n",
    "\"\"\"\n",
    "def do_yes_no_eval(gold_df,gen_df):\n",
    "    print(f\"{CYAN}Yes/No Evaluation{OFF}\")\n",
    "    yes_no_gold_df = gold_df[gold_df['type'] == 'yesno']\n",
    "    yes_no_gen_df = gen_df[gen_df['type'] == 'yesno']\n",
    "    \n",
    "    if DEBUG:\n",
    "        # Gold stats\n",
    "        print_yes_no_info(yes_no_gold_df, \"Gold\")\n",
    "        # Gen Stats\n",
    "        print_yes_no_info(yes_no_gen_df, \"Generated\")\n",
    "\n",
    "    gold_ids,gold_ans,gen_ids,gen_ans = get_col_list(yes_no_gold_df,gen_df,'exact_answer')\n",
    "\n",
    "    # YES \n",
    "    ytp = 0\n",
    "    yfp = 0\n",
    "    yfn = 0\n",
    "    \n",
    "    for i in range (len(gold_ids)):\n",
    "        gold_val = gold_ans[i]\n",
    "        try:\n",
    "            gen_val = gen_ans[gen_ids.index(gold_ids[i])]\n",
    "        except ValueError as e:\n",
    "            if DEBUG:\n",
    "                print(e)\n",
    "            continue\n",
    "        if gen_val:\n",
    "            if gold_val == 'yes':\n",
    "                if gen_val =='yes':\n",
    "                    ytp += 1\n",
    "                elif gen_val =='no':\n",
    "                    yfn += 1\n",
    "                else:\n",
    "                    if DEBUG:\n",
    "                        print(f\"yes question [{gold_ids[i]}] had generated answer {gen_val}\")\n",
    "            elif gold_val == 'no':\n",
    "                if gen_val == 'yes':\n",
    "                    yfp +=1\n",
    "                elif gen_val =='no':\n",
    "                    pass #handled by no f1\n",
    "                else:\n",
    "                    if DEBUG:\n",
    "                        print(f\"no question [{gold_ids[i]}] had generated answer {gen_val}\")\n",
    "            else:\n",
    "                print(f\"GOLDEN answer to yes/no question [{gold_ids[i]}] was {gold_val}\")\n",
    "                \n",
    "        else: # not identified as yes/no question by generated\n",
    "            pass\n",
    "    #sanity check\n",
    "    print (f\"Yes | True Posative: {GREEN}{ytp}{OFF}, False Posative: {GREEN}{yfp}{OFF}, False Negative: {GREEN}{yfn}{OFF}\")\n",
    "    try:\n",
    "        yp = ytp/(ytp + yfp)\n",
    "    except:\n",
    "        yp = 0\n",
    "    try:\n",
    "        yr = ytp/(ytp + yfn)\n",
    "    except:\n",
    "        yr = 0\n",
    "    try:\n",
    "        yf1 = 2 * ((yp * yr)/(yp+yr))\n",
    "    except:\n",
    "        yf1 = 0\n",
    "    print (f'Yes | f1 {GREEN}{yf1}{OFF}, precision {GREEN}{yp}{OFF}, recall {GREEN}{yr}{OFF}')\n",
    "\n",
    "    # NO SIDE\n",
    "    ntp = 0\n",
    "    nfp = 0\n",
    "    nfn = 0\n",
    "\n",
    "    for i in range (len(gold_ids)):\n",
    "        gold_val = gold_ans[i]\n",
    "        try:\n",
    "            gen_val = gen_ans[gen_ids.index(gold_ids[i])]\n",
    "        except ValueError as e:\n",
    "            if DEBUG:\n",
    "                print(e)\n",
    "            continue\n",
    "        if gen_val:\n",
    "            if gold_val == 'no':\n",
    "                if gen_val =='no':\n",
    "                    ntp += 1\n",
    "                elif gen_val =='yes':\n",
    "                    nfn += 1\n",
    "                else:\n",
    "                    if DEBUG:\n",
    "                        print(f\"no question [{gold_ids[i]}] had generated answer {gen_val}\")\n",
    "            elif gold_val == 'yes':\n",
    "                if gen_val == 'no':\n",
    "                    nfp +=1\n",
    "                elif gen_val =='yes':\n",
    "                    pass #handled by no f1\n",
    "                else:\n",
    "                    print(f\"yes question [{gold_ids[i]}] had generated answer {gen_val}\")\n",
    "            else:\n",
    "                if DEBUG:\n",
    "                    print(f\"GOLDEN answer to yes/no question [{gold_ids[i]}] was {gold_val}\")\n",
    "                \n",
    "        else: # not identified as yes/no question by generated\n",
    "            pass\n",
    "    \n",
    "    # sanity check\n",
    "    print (f\"No | True Posative: {GREEN}{ntp}{OFF}, False Posative: {GREEN}{nfp}{OFF}, False Negative: {GREEN}{nfn}{OFF}\")\n",
    "    try:\n",
    "        np = ntp/(ntp + nfp)\n",
    "    except:\n",
    "        np = 0\n",
    "    try:\n",
    "        nr = ntp/(ntp + nfn)\n",
    "    except:\n",
    "        nr = 0\n",
    "    try:\n",
    "        nf1 = 2 * ((np * nr)/(np+nr))\n",
    "    except:\n",
    "        nf1 = 0\n",
    "    print (f'No | f1 {GREEN}{nf1}{OFF}, precision {GREEN}{np}{OFF}, recall {GREEN}{nr}{OFF}')\n",
    "\n",
    "    f1 = (yf1 + nf1)/2 \n",
    "    p = (yp + np)/2 \n",
    "    r = (yr + nr)/2 \n",
    "    print(f\"Overall Yes/No | f1 {GREEN}{f1}{OFF}, precision {GREEN}{p}{OFF}, recall {GREEN}{r}{OFF}\")\n",
    "    print ('\\n')\n",
    "    return yf1,yp,yr,nf1,np,nr,f1,p,r\n",
    "\n",
    "# yes_no_report = do_yes_no_eval(gold_df,gen_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute [[Mean average precision, Geometric mean average precision]], precision, recall, f1 score\n",
    "def do_concepts_eval(gold_df,gen_df):\n",
    "    print(f\"{CYAN}Concepts Evaluation{OFF}\")\n",
    "    gold_ids,gold_cons,gen_ids,gen_cons = get_col_list(gold_df,gen_df,'human_concepts')\n",
    "    num_gen_q_without_cons = 0\n",
    "    num_gold_q_without_cons = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    scores = []\n",
    "    # for each question\n",
    "    for i in range (len(gold_ids)):\n",
    "        gold_val = gold_cons[i]\n",
    "        if not isinstance(gold_val,list) or gold_val == []:\n",
    "            num_gold_q_without_cons += 1\n",
    "            continue\n",
    "        try:\n",
    "            gen_val = gen_cons[gen_ids.index(gold_ids[i])]\n",
    "        except ValueError as e:\n",
    "            if DEBUG:\n",
    "                print(e)\n",
    "            continue\n",
    "        # if concepts are found\n",
    "        if gen_val != []:\n",
    "            # TP is concept in Gold AND Gen\n",
    "            # FP is concept NOT IN GOLD, but YES IN GEN\n",
    "            # FN is concept IN Gold but NOT GEN\n",
    "\n",
    "            # get unique concepts from both gold and gen\n",
    "            unique_gold_cons = set(gold_val[0])\n",
    "            unique_gen_cons = set(gen_val[0])\n",
    "            for val in unique_gold_cons:\n",
    "                if val in unique_gen_cons:\n",
    "                    tp += 1\n",
    "                elif val not in unique_gen_cons:\n",
    "                    fn += 1\n",
    "            for val in unique_gen_cons:\n",
    "                if val not in unique_gold_cons:\n",
    "                    fp += 1\n",
    "\n",
    "            f1,p,r = get_f1_p_r(tp,fp,fn, tag =\"Concepts\")\n",
    "            scores.append((f1,p,r))\n",
    "        else: # There are no concepts retrieved for this document\n",
    "            num_gen_q_without_cons += 1\n",
    "            pass\n",
    "    #sanity check\n",
    "    print(f\"{GREEN}[{len(gold_ids) - num_gold_q_without_cons}/{len(gold_ids)}]{OFF} Questions have human readable concepts in gold dataset\")\n",
    "    print(f\"{GREEN}[{len(gen_ids) - num_gen_q_without_cons}/{len(gen_ids)}]{OFF} Questions have human readable concepts in generated dataset\")\n",
    "\n",
    "    # OVERALL SCORES\n",
    "    f1_sum = p_sum = r_sum = 0\n",
    "    for f1,p,r in scores:\n",
    "        f1_sum += f1\n",
    "        p_sum += p\n",
    "        r_sum += r\n",
    "    f1_sum /= len(scores)\n",
    "    p_sum /= len(scores)\n",
    "    r_sum /= len(scores)\n",
    "\n",
    "    print (f'Concepts mean f1 {GREEN}{f1_sum}{OFF}, precision {GREEN}{p_sum}{OFF}, recall {GREEN}{r_sum}{OFF}')\n",
    "    print ('\\n')\n",
    "    return f1_sum,p_sum,r_sum,scores\n",
    "\n",
    "# concepts_report = do_concepts_eval(gold_df,gen_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1_p_r(tp,fp,fn, tag =\"calculated\"):\n",
    "    if DEBUG:\n",
    "        print (f\"{tag} tp: {tp}, fp: {fp}, fn: {fn}\")\n",
    "    try:\n",
    "        p = tp/(tp + fp)\n",
    "    except:\n",
    "        p = 0\n",
    "    try:\n",
    "        r = tp/(tp + fn)\n",
    "    except:\n",
    "        r = 0\n",
    "    try:\n",
    "        f1 = 2 * ((p * r)/(p+r))\n",
    "    except:\n",
    "        f1 = 0\n",
    "    if DEBUG:\n",
    "        print (f'{tag} f1 {f1}, precision {p}, recall {r}')\n",
    "    return f1,p,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_pmids_eval(gold_df,gen_df):\n",
    "    print(f\"{CYAN}PubMed Documents Evaluation{OFF}\")\n",
    "    # pmids are the pubmed document ids\n",
    "    gold_ids,gold_pmids,gen_ids,gen_pmids = get_col_list(gold_df,gen_df,'documents')\n",
    "    num_gen_q_without_docs = 0\n",
    "    num_gold_q_without_docs = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    scores = []\n",
    "    # for each question\n",
    "    for i in range (len(gold_ids)):\n",
    "        gold_val = gold_pmids[i]\n",
    "        if gold_val == []:\n",
    "            num_gold_q_without_docs += 1\n",
    "            continue\n",
    "        try:\n",
    "            gen_val = gen_pmids[gen_ids.index(gold_ids[i])]\n",
    "        except ValueError as e:\n",
    "            if DEBUG:\n",
    "                print(e)\n",
    "            continue\n",
    "        # if documents are found\n",
    "        if isinstance(gen_val, list) and gen_val != []:\n",
    "            # TP is pmid in Gold AND Gen\n",
    "            # FP is pmid NOT IN GOLD, but YES IN GEN\n",
    "            # FN is pmid IN Gold but NOT GEN\n",
    "\n",
    "            # get unique PMIDs from both gold and gen\n",
    "            try:\n",
    "                unique_gold_pmids = set(gold_val[0])\n",
    "                unique_gen_pmids = set(gen_val[0])\n",
    "            except Exception as e:\n",
    "                continue\n",
    "            for val in unique_gold_pmids:\n",
    "                if val in unique_gen_pmids:\n",
    "                    tp += 1\n",
    "                elif val not in unique_gen_pmids:\n",
    "                    fn += 1\n",
    "            for val in unique_gen_pmids:\n",
    "                if val not in unique_gold_pmids:\n",
    "                    fp += 1\n",
    "\n",
    "            f1,p,r = get_f1_p_r(tp,fp,fn, tag =\"PubMed Documents\")\n",
    "            scores.append((f1,p,r))\n",
    "        else: # There are no documents retrieved for this document\n",
    "            num_gen_q_without_docs += 1\n",
    "            pass\n",
    "    #sanity check\n",
    "    print(f\"{GREEN}[{len(gold_ids) - num_gold_q_without_docs}/{len(gold_ids)}]{OFF} Questions have documents in gold dataset\")\n",
    "    print(f\"{GREEN}[{len(gen_ids) - num_gen_q_without_docs}/{len(gen_ids)}]{OFF} Questions have documents in generated dataset\")\n",
    "\n",
    "    # OVERALL SCORES\n",
    "    f1_sum = p_sum = r_sum = 0\n",
    "    for f1,p,r in scores:\n",
    "        f1_sum += f1\n",
    "        p_sum += p\n",
    "        r_sum += r\n",
    "    f1_sum /= len(scores)\n",
    "    p_sum /= len(scores)\n",
    "    r_sum /= len(scores)\n",
    "\n",
    "    print (f'PubMed Documents mean f1 {GREEN}{f1_sum}{OFF}, precision {GREEN}{p_sum}{OFF}, recall {GREEN}{r_sum}{OFF}')\n",
    "    print ('\\n')\n",
    "    return f1_sum,p_sum,r_sum,scores\n",
    "\n",
    "# pmid_report = do_pmids_eval(gold_df,gen_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use strict and leniant accuracy  (first result, or any result)\n",
    "def do_factoid_eval(gold_df,gen_factoid_path):\n",
    "    print(f\"{CYAN}Factoid Evaluation{OFF}\")\n",
    "    factoid_gold_df = gold_df[gold_df['type'] == 'factoid']\n",
    "    factoid_gen_df = gen_df[gen_df['type'] == 'factoid']\n",
    "\n",
    "    if DEBUG:\n",
    "        print(f\" [{len(factoid_gold_df)}] Gold Factoid Questions\")\n",
    "        print(f\" [{len(factoid_gen_df)}] Generated Factoid Questions\")\n",
    "    gold_ids,gold_ans,gen_ids,gen_ans = get_col_list(factoid_gold_df,gen_df,'exact_answer')\n",
    "\n",
    "    # Use alternative strategy to handle ranked factoid preds\n",
    "    with open(gen_factoid_path, \"r\") as ft_file:\n",
    "        factoid_gen_json = json.load(ft_file)\n",
    "    \n",
    "    gen_factoid_answers = {}\n",
    "    for question in factoid_gen_json[\"questions\"]:\n",
    "        id = question[\"id\"]\n",
    "        if len(id) == 24:\n",
    "            id = id[0:20]\n",
    "        answer = question['exact_answer']\n",
    "        if isinstance(answer,list): \n",
    "            if answer != [] and isinstance(answer[0],list): # handle list in list\n",
    "                answer = [e[0] for e in answer]\n",
    "        gen_factoid_answers[id] = answer\n",
    "\n",
    "    num_gold_q_without_ans = 0\n",
    "    num_strict = 0\n",
    "    num_leniant = 0\n",
    "    num_total = 0\n",
    "    mrrs = []\n",
    "    # for each question\n",
    "    for i in range (len(gold_ids)):\n",
    "        gold_val = gold_ans[i][0]\n",
    "        if gold_val == []:\n",
    "            num_gold_q_without_ans += 1\n",
    "            continue\n",
    "        # trim last 4 digits which get removed for the final bioasq form answers\n",
    "        trimmed_id = gold_ids[i][0:20]\n",
    "        if trimmed_id not in gen_factoid_answers.keys():\n",
    "            if DEBUG:\n",
    "                print(f\"{trimmed_id} wasn't correctly identified as factoid\")\n",
    "            continue\n",
    "        gen_vals = gen_factoid_answers[trimmed_id]\n",
    "        gen_vals_clean = [e.lower().strip() for e in gen_vals]\n",
    "        if DEBUG:\n",
    "            print(gold_val,\" | \",gen_vals)\n",
    "        # accuracy calculations\n",
    "        gold_val_clean = gold_val\n",
    "        num_total += 1\n",
    "        if gold_val_clean == gen_vals_clean[0]: # force lowercase / strip whitespace to help\n",
    "            num_strict += 1\n",
    "            num_leniant += 1\n",
    "        elif gold_val_clean in gen_vals_clean:\n",
    "            num_leniant += 1\n",
    "\n",
    "        # mrr calculations\n",
    "        mrr = 0\n",
    "        r = 0\n",
    "        n = len(gen_vals_clean)\n",
    "        for i in range(1,n+1):\n",
    "            if gen_vals_clean[i-1] == gold_val_clean:\n",
    "                r = i\n",
    "                break\n",
    "        if r != 0:\n",
    "            mrr = 1/n * 1/r \n",
    "            if DEBUG:\n",
    "                print(f\"{trimmed_id} MRR: {mrr}\")\n",
    "        mrrs.append(mrr)\n",
    "\n",
    "    average_mrr = sum(mrrs) / len(mrrs)\n",
    "    leniant_acc = num_leniant/num_total\n",
    "    strict_acc = num_strict/num_total\n",
    "\n",
    "    # sanity check\n",
    "    print(f\"{GREEN}[{len(gold_ids) - num_gold_q_without_ans}/{len(gold_ids)}]{OFF} Factoid questions have answers in gold dataset\")\n",
    "    print(f\"{GREEN}[{num_total}/{len(gen_factoid_answers)}]{OFF} Factoid questions have answers in generated dataset\")\n",
    "    print(f\"Leniant Accuracy: {GREEN}{leniant_acc}{OFF}, Strict Accuracy: {GREEN}{strict_acc}{OFF}, Mean Reciprocal Rank (MRR): {GREEN}{average_mrr}{OFF}\")\n",
    "    print ('\\n')\n",
    "    return leniant_acc,strict_acc,average_mrr,mrrs\n",
    "\n",
    "# do_factoid_eval(gold_df,'tmp/qa/factoid/BioASQform_BioASQ-answer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_list_eval(gold_df,gen_df):\n",
    "    print(f\"{CYAN}List Evaluation{OFF}\")\n",
    "    list_gold_df = gold_df[gold_df['type'] == 'list']\n",
    "    list_gen_df = gen_df[gen_df['type'] == 'list']\n",
    "\n",
    "    if DEBUG:\n",
    "        print(f\" [{len(list_gold_df)}] Gold List Questions\")\n",
    "        print(f\" [{len(list_gen_df)}] Generated List Questions\")\n",
    "    \n",
    "    gold_ids,gold_ans,gen_ids,gen_ans = get_col_list(list_gold_df,gen_df,'exact_answer')\n",
    "    num_gen_q_without_ans = 0\n",
    "    num_gold_q_without_ans = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    num_gen = 0\n",
    "    scores = []\n",
    "    # for each question\n",
    "    for i in range (len(gold_ids)):\n",
    "        gold_val = gold_ans[i]\n",
    "        if gold_val == []:\n",
    "            num_gold_q_without_ans += 1\n",
    "            continue\n",
    "        try:\n",
    "            gen_val = gen_ans[gen_ids.index(gold_ids[i])]\n",
    "        except ValueError as e:\n",
    "            if DEBUG:\n",
    "                print(e)\n",
    "            continue\n",
    "        # if answers are found\n",
    "        if gen_val != None: # List is only able to find a single item list\n",
    "            # TP is answer in Gold AND Gen\n",
    "            # FP is answer NOT IN GOLD, but YES IN GEN\n",
    "            # FN is answer IN Gold but NOT GEN\n",
    "            gold_list = gold_val[0]\n",
    "            for val in gold_list:\n",
    "                if val in gen_val:\n",
    "                    tp += 1\n",
    "                elif val not in gen_val:\n",
    "                    fn += 1\n",
    "            for val in gen_val:\n",
    "                if val not in gold_list:\n",
    "                    fp += 1\n",
    "            num_gen += 1\n",
    "            f1,p,r = get_f1_p_r(tp,fp,fn, tag =\"List Questions\")\n",
    "            scores.append((f1,p,r))\n",
    "    #sanity check\n",
    "    print(f\"{GREEN}[{len(gold_ids) - num_gold_q_without_ans}/{len(gold_ids)}]{OFF} List questions have answers in gold dataset\")\n",
    "    print(f\"{GREEN}[{num_gen}/{len(list_gen_df)}]{OFF} List questions have answers in generated dataset\")\n",
    "\n",
    "    # OVERALL SCORES\n",
    "    f1_sum = p_sum = r_sum = 0\n",
    "    for f1,p,r in scores:\n",
    "        f1_sum += f1\n",
    "        p_sum += p\n",
    "        r_sum += r\n",
    "    f1_sum /= len(scores)\n",
    "    p_sum /= len(scores)\n",
    "    r_sum /= len(scores)\n",
    "\n",
    "    print (f'List Questions mean f1 {GREEN}{f1_sum}{OFF}, precision {GREEN}{p_sum}{OFF}, recall {GREEN}{r_sum}{OFF}')\n",
    "    print ('\\n')\n",
    "    return f1_sum,p_sum,r_sum,scores\n",
    "\n",
    "# list_report = do_list_eval(gold_df,gen_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_gold_qu_output(gold_df, gen_folder, gen_xml_name=\"bioasq_qa.xml\"):\n",
    "    qu_generated = gen_folder + \"/ir/input/\" + gen_xml_name\n",
    "    new_file_name = qu_generated.replace(\".xml\", \"_GOLD.xml\")\n",
    "\n",
    "    fileTree = et.parse(qu_generated)\n",
    "    if fileTree:\n",
    "        root = fileTree.getroot()\n",
    "        questions = root.findall('Q')\n",
    "        for question in questions:\n",
    "            id = question.attrib.get(\"id\")\n",
    "            qp = question.find(\"QP\")\n",
    "            # remove type and entities\n",
    "            qp.clear()\n",
    "            # type is the fifth element in the row\n",
    "            gold_type = gold_df.loc[gold_df['id'] == id].values[0][4]\n",
    "            type_ele = et.SubElement(qp,'Type')\n",
    "            type_ele.text = gold_type\n",
    "            gold_concepts = gold_df.loc[gold_df['id'] == id].values[0][7]\n",
    "            ent_list = []\n",
    "            qp_query = et.SubElement(qp,'Query')\n",
    "            if not isinstance(gold_concepts,list):\n",
    "                if DEBUG:\n",
    "                    print(f\"Question [{id}] has no golden human_concepts [{type(gold_concepts)}]\")\n",
    "                continue\n",
    "            for ent in gold_concepts:\n",
    "                ent_list.append(str(ent))\n",
    "                qp_en = et.SubElement(qp,'Entities') \n",
    "                qp_en.text = str(ent)\n",
    "            qp_query.text = str(' '.join(ent_list))\n",
    "        tree = et.ElementTree(root)\n",
    "        os.makedirs(os.path.dirname(new_file_name), exist_ok=True)\n",
    "        print(f\"Writing gold QU output / IR input to {new_file_name}\")\n",
    "        tree.write(new_file_name, pretty_print=True)\n",
    "    return new_file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_gold_ir_output(gold_df, gen_folder, gen_xml_name=\"bioasq_qa.xml\"):\n",
    "    ir_generated = gen_folder + \"/ir/output/\" + gen_xml_name\n",
    "    new_file_name = ir_generated.replace(\".xml\", \"_GOLD.xml\")\n",
    "\n",
    "    fileTree = et.parse(ir_generated)\n",
    "    if fileTree:\n",
    "        root = fileTree.getroot()\n",
    "        questions = root.findall('Q')\n",
    "        for question in questions:\n",
    "            id = question.attrib.get(\"id\")\n",
    "            original_question = question.text\n",
    "            if DEBUG:\n",
    "                print(original_question)\n",
    "            ir = question.find(\"IR\")\n",
    "            # remove original generated articles\n",
    "            ir.clear()\n",
    "\n",
    "            gold_abstracts = gold_df.loc[gold_df['id'] == id].values[0][8]\n",
    "            gold_titles = gold_df.loc[gold_df['id'] == id].values[0][9]\n",
    "            # system just using top abstract atm\n",
    "            if isinstance(gold_abstracts,list):\n",
    "                gold_abstract = gold_abstracts[0]\n",
    "            else:\n",
    "                gold_abstract = \"\"\n",
    "            if isinstance(gold_titles,list):\n",
    "                gold_title = gold_titles[0]\n",
    "            else:\n",
    "                gold_title = \"\"\n",
    "            # fill result\n",
    "            result_tag = et.SubElement(ir, \"Result\")\n",
    "            gold_df.loc[gold_df['id'] == id].values[0][8]\n",
    "            pmid = gold_df.loc[gold_df['id'] == id].values[0][1][0]\n",
    "            result_tag.set(\"PMID\", pmid)\n",
    "            title = et.SubElement(result_tag, \"Title\")\n",
    "            title.text = gold_title\n",
    "            abstract = et.SubElement(result_tag, \"Abstract\")\n",
    "            abstract.text = gold_abstract\n",
    "        tree = et.ElementTree(root)\n",
    "        os.makedirs(os.path.dirname(new_file_name), exist_ok=True)\n",
    "        print(f\"Writing gold QA input / IR output to {new_file_name}\")\n",
    "        tree.write(new_file_name, pretty_print=True)\n",
    "    return new_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_intermediary_datasets(gold_df,qu_output,ir_output):\n",
    "    # QU: replace concepts with human_concepts and type with gold type\n",
    "    gold_qu_output = gen_gold_qu_output(gold_df,qu_output)\n",
    "    # IR: Replace IR with full_abstract in document format\n",
    "    gold_ir_output = gen_gold_ir_output(gold_df,ir_output)\n",
    "\n",
    "    return gold_qu_output, gold_ir_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_the_tests(gold_dataset_path, generation_folder_path, gen_xml_name):\n",
    "    factoid_path = generation_folder_path + \"/qa/factoid/BioASQform_BioASQ-answer.json\"\n",
    "    generated_qu = generation_folder_path + \"/ir/output/\" + gen_xml_name\n",
    "\n",
    "    with open(gold_dataset_path, \"r\") as f:\n",
    "        gold_data = json.loads(f.read())\n",
    "    # load and flatten data\n",
    "    gold_df = pd.json_normalize(gold_data, record_path=\"questions\")\n",
    "    # get gold df\n",
    "    gold_df[\"documents\"] = gold_df[\"documents\"].apply(get_pmid)\n",
    "    # get generated df\n",
    "    gen_df = parse_xml(generated_qu, generation_folder_path + \"/qa\")\n",
    "\n",
    "    # do tests\n",
    "    concepts_report = do_concepts_eval(gold_df, gen_df)\n",
    "    pmids_report = do_pmids_eval(gold_df, gen_df)\n",
    "    gold_type = gold_df[\"type\"].to_numpy()\n",
    "    gen_type = gen_df[\"type\"].to_numpy()\n",
    "    try:\n",
    "        print(f\"{CYAN}Type Evaluation{OFF}\")\n",
    "        print(classification_report(gold_type, gen_type))\n",
    "        type_report = classification_report(gold_type, gen_type, output_dict=True)\n",
    "    except:\n",
    "        type_report = f\"TypeReport: Found input variables with inconsistent numbers of samples: [{len(gold_type)}] [{len(gen_type)}]\"\n",
    "    yes_no_report = do_yes_no_eval(gold_df, gen_df)\n",
    "    factoid_report = do_factoid_eval(gold_df, gen_df, factoid_path)\n",
    "    list_report = do_list_eval(gold_df, gen_df)\n",
    "\n",
    "    test_results = (\n",
    "        concepts_report,\n",
    "        pmids_report,\n",
    "        type_report,\n",
    "        yes_no_report,\n",
    "        factoid_report,\n",
    "        list_report,\n",
    "    )\n",
    "    save_results(test_results, generation_folder_path)\n",
    "    return test_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(test_results,save_path):\n",
    "    t = time.localtime()\n",
    "    date = time.strftime('%b-%d-%Y', t)\n",
    "    concepts_report,pmids_report,type_report,yes_no_report,factoid_report,list_report = test_results\n",
    "    results_folder = save_path + \"/test_results/\" + date \n",
    "    timestamp = time.strftime('%H%M%S', t)\n",
    "    if not os.path.exists(results_folder):\n",
    "        os.makedirs(results_folder)\n",
    "\n",
    "    # Save concepts\n",
    "    con_name = results_folder + f\"/concepts-{timestamp}.csv\"\n",
    "    with open(con_name,'w+') as f:\n",
    "        f1_sum,p_sum,r_sum,scores = concepts_report\n",
    "        f.write(\"Average f1 score,Average precision,Average Recall\\n\")\n",
    "        f.write(f\"{f1_sum},{p_sum},{r_sum}\\n\")\n",
    "        f.write(\"f1,precision,recall\\n\")\n",
    "        for f1,p,r in scores:\n",
    "            f.write(f\"{f1},{p},{r}\\n\")\n",
    "    print(f\"Saved concepts info to {con_name}\")\n",
    "\n",
    "    # Save pmids\n",
    "    pmid_name = results_folder + f\"/pmids-{timestamp}.csv\"\n",
    "    with open(pmid_name,'w+') as f:\n",
    "        f1_sum,p_sum,r_sum,scores = pmids_report\n",
    "        f.write(\"Average f1 score,Average precision,Average Recall\\n\")\n",
    "        f.write(f\"{f1_sum},{p_sum},{r_sum}\\n\")\n",
    "        f.write(\"f1,precision,recall\\n\")\n",
    "        for f1,p,r in scores:\n",
    "            f.write(f\"{f1},{p},{r}\\n\")\n",
    "    print(f\"Saved pmid info to {pmid_name}\")\n",
    "\n",
    "\n",
    "    # Save type report\n",
    "    type_name = results_folder + f\"/type-{timestamp}.json\"\n",
    "    with open(type_name,'w+') as f:\n",
    "        json.dump(type_report,f,indent=2)\n",
    "    print(f\"Saved type info to {type_name}\")\n",
    "\n",
    "\n",
    "    # Save yes/no\n",
    "    yesno_name = results_folder + f\"/yesno-{timestamp}.csv\"\n",
    "    with open(yesno_name,'w+') as f: \n",
    "        yf1,yp,yr,nf1,np,nr,f1,p,r = yes_no_report\n",
    "        f.write(\"Yes/No f1 score,Yes/No precision ,Yes/No recall\\n\")\n",
    "        f.write(f\"{f1},{p},{r}\\n\")\n",
    "        f.write(\"Yes f1 score,Yes precision ,Yes recall\\n\")\n",
    "        f.write(f\"{yf1},{yp},{yr}\\n\")\n",
    "        f.write(\"No f1 score,No precision ,No recall\\n\")\n",
    "        f.write(f\"{nf1},{np},{nr}\\n\")\n",
    "    print(f\"Saved yes/no info to {yesno_name}\")\n",
    "\n",
    "    # Save factoids\n",
    "    fact_name = results_folder + f\"/factoid-{timestamp}.csv\"\n",
    "    with open(fact_name,'w+') as f:\n",
    "        leniant_acc,strict_acc,average_mrr,mrrs = factoid_report\n",
    "        f.write(\"Factoid leniant accuracy,Factoid strict accuracy,Factoid average mrr\\n\")\n",
    "        f.write(f\"{leniant_acc},{strict_acc},{average_mrr}\\n\")\n",
    "        f.write(\"mrr\\n\")\n",
    "        for mrr in mrrs:\n",
    "            f.write(f\"{mrr}\\n\")\n",
    "    print(f\"Saved factoid info to {fact_name}\")\n",
    "\n",
    "    # Save list\n",
    "    list_name = results_folder + f\"/list-{timestamp}.csv\"\n",
    "    with open(list_name,'w+') as f:\n",
    "        f1_sum,p_sum,r_sum,scores = list_report\n",
    "        f.write(\"Average f1 score,Average precision,Average Recall\\n\")\n",
    "        f.write(f\"{f1_sum},{p_sum},{r_sum}\\n\")\n",
    "        f.write(\"f1,precision,recall\\n\")\n",
    "        for f1,p,r in scores:\n",
    "            f.write(f\"{f1},{p},{r}\\n\")\n",
    "    print(f\"Saved list info to {list_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the golden answer dataframe\n",
    "DEBUG = False\n",
    "golden_dataset_path = \"testing_datasets/augmented_concepts_abstracts_titles.json\"\n",
    "# gen_folder = \"tmp/small_batch\"\n",
    "gen_folder = \"tmp\"\n",
    "\n",
    "# xml_name = \"bioasq_qa_SMALL.xml\"\n",
    "xml_name = \"bioasq_qa.xml\"\n",
    "\n",
    "test_results = run_all_the_tests(golden_dataset_path,gen_folder,xml_name)\n",
    "save_results(test_results,gen_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ngolden_dataset_path = \"testing_datasets/augmented_concepts_abstracts_titles.json\"\\ngen_folder = \"tmp\"\\nxml_name = \"bioasq_qa.xml\"\\n\\ntest_results = run_all_the_tests(golden_dataset_path, gen_folder, xml_name)\\nsave_results(test_results, gen_folder)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lxml import etree as et\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "from utils import *\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "\n",
    "def get_pmid(docs):\n",
    "    documents = [document.split(\"/\")[-1] for document in docs]\n",
    "    return documents\n",
    "\n",
    "\n",
    "def get_answers(answers_files):\n",
    "    all_answers = dict()\n",
    "    for a in answers_files:\n",
    "        with open(a, \"r\") as f:\n",
    "            d = json.loads(f.read())\n",
    "            if DEBUG:\n",
    "                print(f\"{len(d)} answers found in {a}\")\n",
    "            for key, value in d.items():\n",
    "                if key in all_answers.keys():\n",
    "                    print(f\"MULTIPLE ANSWERS FOR {key}\")\n",
    "                if isinstance(value, list):\n",
    "                    all_answers[key] = value[\n",
    "                        0\n",
    "                    ]  # get the first value which is answer not prediction for the yes/no\n",
    "                else:\n",
    "                    all_answers[key] = value\n",
    "    return all_answers\n",
    "\n",
    "\n",
    "def get_three_files(a_dir):\n",
    "    return [\n",
    "        a_dir + \"/factoid/predictions.json\",\n",
    "        a_dir + \"/list/predictions.json\",\n",
    "        a_dir + \"/yesno/predictions.json\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_col_list(gold_df, gen_df, col):\n",
    "    gold_col = gold_df.loc[:, [\"id\", col]].copy()\n",
    "    gen_col = gen_df.loc[:, [\"id\", col]].copy()\n",
    "\n",
    "    gold = gold_col.to_dict(orient=\"list\")\n",
    "    gen = gen_col.to_dict(orient=\"list\")\n",
    "    gen_ids = gen[\"id\"]\n",
    "    gen_vals = gen[col]\n",
    "    gold_ids = gold[\"id\"]\n",
    "    gold_vals = gold[col]\n",
    "    return gold_ids, gold_vals, gen_ids, gen_vals\n",
    "\n",
    "\n",
    "def parse_xml(xml_file, dir_for_qa):\n",
    "    no_answers = 0\n",
    "    # get answers\n",
    "    qa_answers = get_answers(get_three_files(dir_for_qa))\n",
    "    # get ir and qu\n",
    "    df_cols = [\n",
    "        \"id\",\n",
    "        \"human_concepts\",\n",
    "        \"documents\",\n",
    "        \"full_abstracts\",\n",
    "        \"titles\",\n",
    "        \"type\",\n",
    "        \"exact_answer\",\n",
    "    ]\n",
    "    xtree = et.parse(xml_file)\n",
    "    xroot = xtree.getroot()\n",
    "    rows = []\n",
    "    for question in xroot:\n",
    "        id = question.attrib.get(\"id\")\n",
    "        ir = question.find(\"IR\")\n",
    "        qp = question.find(\"QP\")\n",
    "        concepts = [e.text for e in qp.findall(\"Entities\")]\n",
    "        qa_type = qp.find(\"Type\").text\n",
    "        titles = [e.find(\"Title\").text for e in ir.findall(\"Result\")]\n",
    "        abstracts = [e.find(\"Abstract\").text for e in ir.findall(\"Result\")]\n",
    "        pmids = [e.get(\"PMID\") for e in ir.findall(\"Result\")]\n",
    "        exact_answer = qa_answers[id] if id in qa_answers else None\n",
    "        if DEBUG and not exact_answer:\n",
    "            print(f\"id [{id}] has no answer\")\n",
    "            no_answers += 1\n",
    "        rows.append(\n",
    "            {\n",
    "                \"id\": id,\n",
    "                \"human_concepts\": concepts,\n",
    "                \"documents\": pmids,\n",
    "                \"full_abstracts\": abstracts,\n",
    "                \"titles\": titles,\n",
    "                \"type\": qa_type,\n",
    "                \"exact_answer\": exact_answer,\n",
    "            }\n",
    "        )\n",
    "    out_df = pd.DataFrame(rows, columns=df_cols)\n",
    "    if DEBUG:\n",
    "        print(\n",
    "            f\"{GREEN}[{no_answers}/{len(out_df)}]{OFF} {WHITE}questions had answers{OFF}\"\n",
    "        )\n",
    "    return out_df\n",
    "\n",
    "\n",
    "def print_yes_no_info(df, tag):\n",
    "    print(tag)\n",
    "    print(f\" [{len(df)}] {tag} Yes/No Questions\")\n",
    "    yes_df = df[df[\"exact_answer\"] == \"yes\"]\n",
    "    no_df = df[df[\"exact_answer\"] == \"no\"]\n",
    "    print(f\" [{len(yes_df)}] {tag} Yes Questions\")\n",
    "    print(f\" [{len(no_df)}] {tag} No Questions\")\n",
    "\n",
    "\n",
    "\"\"\" f1 Yes\n",
    "    tp is gen 'yes' | gold 'yes'\n",
    "    fp is gen 'yes' | gold 'no'\n",
    "    fn is gen 'no' |  gold 'yes'\n",
    "\n",
    "    f1 No\n",
    "    tp is gen 'no' | gold 'no'\n",
    "    fp is gen 'no' | gold 'yes'\n",
    "    fn is gen 'yes' |  gold 'no'\n",
    "\n",
    "    IGNORE if the predicted type is yes/no but gold type is different\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def do_yes_no_eval(gold_df, gen_df):\n",
    "    print(f\"{CYAN}Yes/No Evaluation{OFF}\")\n",
    "    yes_no_gold_df = gold_df[gold_df[\"type\"] == \"yesno\"]\n",
    "    yes_no_gen_df = gen_df[gen_df[\"type\"] == \"yesno\"]\n",
    "\n",
    "    if DEBUG:\n",
    "        # Gold stats\n",
    "        print_yes_no_info(yes_no_gold_df, \"Gold\")\n",
    "        # Gen Stats\n",
    "        print_yes_no_info(yes_no_gen_df, \"Generated\")\n",
    "\n",
    "    gold_ids, gold_ans, gen_ids, gen_ans = get_col_list(\n",
    "        yes_no_gold_df, gen_df, \"exact_answer\"\n",
    "    )\n",
    "\n",
    "    # YES\n",
    "    ytp = 0\n",
    "    yfp = 0\n",
    "    yfn = 0\n",
    "\n",
    "    for i in range(len(gold_ids)):\n",
    "        gold_val = gold_ans[i]\n",
    "        try:\n",
    "            gen_val = gen_ans[gen_ids.index(gold_ids[i])]\n",
    "        except ValueError as e:\n",
    "            if DEBUG:\n",
    "                print(e)\n",
    "            continue\n",
    "        if gen_val:\n",
    "            if gold_val == \"yes\":\n",
    "                if gen_val == \"yes\":\n",
    "                    ytp += 1\n",
    "                elif gen_val == \"no\":\n",
    "                    yfn += 1\n",
    "                else:\n",
    "                    if DEBUG:\n",
    "                        print(\n",
    "                            f\"yes question [{gold_ids[i]}] had generated answer {gen_val}\"\n",
    "                        )\n",
    "            elif gold_val == \"no\":\n",
    "                if gen_val == \"yes\":\n",
    "                    yfp += 1\n",
    "                elif gen_val == \"no\":\n",
    "                    pass  # handled by no f1\n",
    "                else:\n",
    "                    if DEBUG:\n",
    "                        print(\n",
    "                            f\"no question [{gold_ids[i]}] had generated answer {gen_val}\"\n",
    "                        )\n",
    "            else:\n",
    "                print(\n",
    "                    f\"GOLDEN answer to yes/no question [{gold_ids[i]}] was {gold_val}\"\n",
    "                )\n",
    "\n",
    "        else:  # not identified as yes/no question by generated\n",
    "            pass\n",
    "    # sanity check\n",
    "    print(\n",
    "        f\"Yes | True Posative: {GREEN}{ytp}{OFF}, False Posative: {GREEN}{yfp}{OFF}, False Negative: {GREEN}{yfn}{OFF}\"\n",
    "    )\n",
    "    try:\n",
    "        yp = ytp / (ytp + yfp)\n",
    "    except:\n",
    "        yp = 0\n",
    "    try:\n",
    "        yr = ytp / (ytp + yfn)\n",
    "    except:\n",
    "        yr = 0\n",
    "    try:\n",
    "        yf1 = 2 * ((yp * yr) / (yp + yr))\n",
    "    except:\n",
    "        yf1 = 0\n",
    "    print(\n",
    "        f\"Yes | f1 {GREEN}{yf1}{OFF}, precision {GREEN}{yp}{OFF}, recall {GREEN}{yr}{OFF}\"\n",
    "    )\n",
    "\n",
    "    # NO SIDE\n",
    "    ntp = 0\n",
    "    nfp = 0\n",
    "    nfn = 0\n",
    "\n",
    "    for i in range(len(gold_ids)):\n",
    "        gold_val = gold_ans[i]\n",
    "        try:\n",
    "            gen_val = gen_ans[gen_ids.index(gold_ids[i])]\n",
    "        except ValueError as e:\n",
    "            if DEBUG:\n",
    "                print(e)\n",
    "            continue\n",
    "        if gen_val:\n",
    "            if gold_val == \"no\":\n",
    "                if gen_val == \"no\":\n",
    "                    ntp += 1\n",
    "                elif gen_val == \"yes\":\n",
    "                    nfn += 1\n",
    "                else:\n",
    "                    if DEBUG:\n",
    "                        print(\n",
    "                            f\"no question [{gold_ids[i]}] had generated answer {gen_val}\"\n",
    "                        )\n",
    "            elif gold_val == \"yes\":\n",
    "                if gen_val == \"no\":\n",
    "                    nfp += 1\n",
    "                elif gen_val == \"yes\":\n",
    "                    pass  # handled by no f1\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"yes question [{gold_ids[i]}] had generated answer {gen_val}\"\n",
    "                    )\n",
    "            else:\n",
    "                if DEBUG:\n",
    "                    print(\n",
    "                        f\"GOLDEN answer to yes/no question [{gold_ids[i]}] was {gold_val}\"\n",
    "                    )\n",
    "\n",
    "        else:  # not identified as yes/no question by generated\n",
    "            pass\n",
    "\n",
    "    # sanity check\n",
    "    print(\n",
    "        f\"No | True Posative: {GREEN}{ntp}{OFF}, False Posative: {GREEN}{nfp}{OFF}, False Negative: {GREEN}{nfn}{OFF}\"\n",
    "    )\n",
    "    try:\n",
    "        np = ntp / (ntp + nfp)\n",
    "    except:\n",
    "        np = 0\n",
    "    try:\n",
    "        nr = ntp / (ntp + nfn)\n",
    "    except:\n",
    "        nr = 0\n",
    "    try:\n",
    "        nf1 = 2 * ((np * nr) / (np + nr))\n",
    "    except:\n",
    "        nf1 = 0\n",
    "    print(\n",
    "        f\"No | f1 {GREEN}{nf1}{OFF}, precision {GREEN}{np}{OFF}, recall {GREEN}{nr}{OFF}\"\n",
    "    )\n",
    "\n",
    "    f1 = (yf1 + nf1) / 2\n",
    "    p = (yp + np) / 2\n",
    "    r = (yr + nr) / 2\n",
    "    print(\n",
    "        f\"Overall Yes/No | f1 {GREEN}{f1}{OFF}, precision {GREEN}{p}{OFF}, recall {GREEN}{r}{OFF}\"\n",
    "    )\n",
    "    print(\"\\n\")\n",
    "    return yf1, yp, yr, nf1, np, nr, f1, p, r\n",
    "\n",
    "\n",
    "# yes_no_report = do_yes_no_eval(gold_df,gen_df)\n",
    "# Compute [[Mean average precision, Geometric mean average precision]], precision, recall, f1 score\n",
    "def do_concepts_eval(gold_df, gen_df):\n",
    "    print(f\"{CYAN}Concepts Evaluation{OFF}\")\n",
    "    gold_ids, gold_cons, gen_ids, gen_cons = get_col_list(\n",
    "        gold_df, gen_df, \"human_concepts\"\n",
    "    )\n",
    "    num_gen_q_without_cons = 0\n",
    "    num_gold_q_without_cons = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    scores = []\n",
    "    # for each question\n",
    "    for i in range(len(gold_ids)):\n",
    "        gold_val = gold_cons[i]\n",
    "        if not isinstance(gold_val, list) or gold_val == []:\n",
    "            num_gold_q_without_cons += 1\n",
    "            continue\n",
    "        try:\n",
    "            gen_val = gen_cons[gen_ids.index(gold_ids[i])]\n",
    "        except ValueError as e:\n",
    "            if DEBUG:\n",
    "                print(e)\n",
    "            continue\n",
    "        # if concepts are found\n",
    "        if gen_val != []:\n",
    "            # TP is concept in Gold AND Gen\n",
    "            # FP is concept NOT IN GOLD, but YES IN GEN\n",
    "            # FN is concept IN Gold but NOT GEN\n",
    "\n",
    "            # get unique concepts from both gold and gen\n",
    "            unique_gold_cons = set(gold_val[0])\n",
    "            unique_gen_cons = set(gen_val[0])\n",
    "            for val in unique_gold_cons:\n",
    "                if val in unique_gen_cons:\n",
    "                    tp += 1\n",
    "                elif val not in unique_gen_cons:\n",
    "                    fn += 1\n",
    "            for val in unique_gen_cons:\n",
    "                if val not in unique_gold_cons:\n",
    "                    fp += 1\n",
    "\n",
    "            f1, p, r = get_f1_p_r(tp, fp, fn, tag=\"Concepts\")\n",
    "            scores.append((f1, p, r))\n",
    "        else:  # There are no concepts retrieved for this document\n",
    "            num_gen_q_without_cons += 1\n",
    "            pass\n",
    "    # sanity check\n",
    "    print(\n",
    "        f\"{GREEN}[{len(gold_ids) - num_gold_q_without_cons}/{len(gold_ids)}]{OFF} Questions have human readable concepts in gold dataset\"\n",
    "    )\n",
    "    print(\n",
    "        f\"{GREEN}[{len(gen_ids) - num_gen_q_without_cons}/{len(gen_ids)}]{OFF} Questions have human readable concepts in generated dataset\"\n",
    "    )\n",
    "\n",
    "    # OVERALL SCORES\n",
    "    f1_sum = p_sum = r_sum = 0\n",
    "    for f1, p, r in scores:\n",
    "        f1_sum += f1\n",
    "        p_sum += p\n",
    "        r_sum += r\n",
    "    f1_sum /= len(scores)\n",
    "    p_sum /= len(scores)\n",
    "    r_sum /= len(scores)\n",
    "\n",
    "    print(\n",
    "        f\"Concepts mean f1 {GREEN}{f1_sum}{OFF}, precision {GREEN}{p_sum}{OFF}, recall {GREEN}{r_sum}{OFF}\"\n",
    "    )\n",
    "    print(\"\\n\")\n",
    "    return f1_sum, p_sum, r_sum, scores\n",
    "\n",
    "\n",
    "# concepts_report = do_concepts_eval(gold_df,gen_df)\n",
    "def get_f1_p_r(tp, fp, fn, tag=\"calculated\"):\n",
    "    if DEBUG:\n",
    "        print(f\"{tag} tp: {tp}, fp: {fp}, fn: {fn}\")\n",
    "    try:\n",
    "        p = tp / (tp + fp)\n",
    "    except:\n",
    "        p = 0\n",
    "    try:\n",
    "        r = tp / (tp + fn)\n",
    "    except:\n",
    "        r = 0\n",
    "    try:\n",
    "        f1 = 2 * ((p * r) / (p + r))\n",
    "    except:\n",
    "        f1 = 0\n",
    "    if DEBUG:\n",
    "        print(f\"{tag} f1 {f1}, precision {p}, recall {r}\")\n",
    "    return f1, p, r\n",
    "\n",
    "\n",
    "def do_pmids_eval(gold_df, gen_df):\n",
    "    print(f\"{CYAN}PubMed Documents Evaluation{OFF}\")\n",
    "    # pmids are the pubmed document ids\n",
    "    gold_ids, gold_pmids, gen_ids, gen_pmids = get_col_list(\n",
    "        gold_df, gen_df, \"documents\"\n",
    "    )\n",
    "    num_gen_q_without_docs = 0\n",
    "    num_gold_q_without_docs = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    scores = []\n",
    "    # for each question\n",
    "    for i in range(len(gold_ids)):\n",
    "        gold_val = gold_pmids[i]\n",
    "        if gold_val == []:\n",
    "            num_gold_q_without_docs += 1\n",
    "            continue\n",
    "        try:\n",
    "            gen_val = gen_pmids[gen_ids.index(gold_ids[i])]\n",
    "        except ValueError as e:\n",
    "            if DEBUG:\n",
    "                print(e)\n",
    "            continue\n",
    "        # if documents are found\n",
    "        if isinstance(gen_val, list) and gen_val != []:\n",
    "            # TP is pmid in Gold AND Gen\n",
    "            # FP is pmid NOT IN GOLD, but YES IN GEN\n",
    "            # FN is pmid IN Gold but NOT GEN\n",
    "\n",
    "            # get unique PMIDs from both gold and gen\n",
    "            unique_gold_pmids = set(gold_val[0])\n",
    "            unique_gen_pmids = set(gen_val[0])\n",
    "            for val in unique_gold_pmids:\n",
    "                if val in unique_gen_pmids:\n",
    "                    tp += 1\n",
    "                elif val not in unique_gen_pmids:\n",
    "                    fn += 1\n",
    "            for val in unique_gen_pmids:\n",
    "                if val not in unique_gold_pmids:\n",
    "                    fp += 1\n",
    "\n",
    "            f1, p, r = get_f1_p_r(tp, fp, fn, tag=\"PubMed Documents\")\n",
    "            scores.append((f1, p, r))\n",
    "        else:  # There are no documents retrieved for this document\n",
    "            num_gen_q_without_docs += 1\n",
    "            pass\n",
    "    # sanity check\n",
    "    print(\n",
    "        f\"{GREEN}[{len(gold_ids) - num_gold_q_without_docs}/{len(gold_ids)}]{OFF} Questions have documents in gold dataset\"\n",
    "    )\n",
    "    print(\n",
    "        f\"{GREEN}[{len(gen_ids) - num_gen_q_without_docs}/{len(gen_ids)}]{OFF} Questions have documents in generated dataset\"\n",
    "    )\n",
    "\n",
    "    # OVERALL SCORES\n",
    "    f1_sum = p_sum = r_sum = 0\n",
    "    for f1, p, r in scores:\n",
    "        f1_sum += f1\n",
    "        p_sum += p\n",
    "        r_sum += r\n",
    "    f1_sum /= len(scores)\n",
    "    p_sum /= len(scores)\n",
    "    r_sum /= len(scores)\n",
    "\n",
    "    print(\n",
    "        f\"PubMed Documents mean f1 {GREEN}{f1_sum}{OFF}, precision {GREEN}{p_sum}{OFF}, recall {GREEN}{r_sum}{OFF}\"\n",
    "    )\n",
    "    print(\"\\n\")\n",
    "    return f1_sum, p_sum, r_sum, scores\n",
    "\n",
    "\n",
    "# pmid_report = do_pmids_eval(gold_df,gen_df)\n",
    "# We use strict and leniant accuracy  (first result, or any result)\n",
    "def do_factoid_eval(gold_df, gen_df, gen_factoid_path):\n",
    "    print(f\"{CYAN}Factoid Evaluation{OFF}\")\n",
    "    factoid_gold_df = gold_df[gold_df[\"type\"] == \"factoid\"]\n",
    "    factoid_gen_df = gen_df[gen_df[\"type\"] == \"factoid\"]\n",
    "\n",
    "    if DEBUG:\n",
    "        print(f\" [{len(factoid_gold_df)}] Gold Factoid Questions\")\n",
    "        print(f\" [{len(factoid_gen_df)}] Generated Factoid Questions\")\n",
    "    gold_ids, gold_ans, gen_ids, gen_ans = get_col_list(\n",
    "        factoid_gold_df, gen_df, \"exact_answer\"\n",
    "    )\n",
    "\n",
    "    # Use alternative strategy to handle ranked factoid preds\n",
    "    with open(gen_factoid_path, \"r\") as ft_file:\n",
    "        factoid_gen_json = json.load(ft_file)\n",
    "\n",
    "    gen_factoid_answers = {}\n",
    "    for question in factoid_gen_json[\"questions\"]:\n",
    "        id = question[\"id\"]\n",
    "        if len(id) == 24:\n",
    "            id = id[0:20]\n",
    "        answer = question[\"exact_answer\"]\n",
    "        if answer == []:\n",
    "            answer = \"empty\"\n",
    "        if isinstance(answer, list):\n",
    "            if isinstance(answer[0], list):  # handle list in list\n",
    "                answer = [e[0] for e in answer]\n",
    "        gen_factoid_answers[id] = answer\n",
    "\n",
    "    num_gold_q_without_ans = 0\n",
    "    num_strict = 0\n",
    "    num_leniant = 0\n",
    "    num_total = 0\n",
    "    mrrs = []\n",
    "    # for each question\n",
    "    for i in range(len(gold_ids)):\n",
    "        gold_val = gold_ans[i][0]\n",
    "        if gold_val == []:\n",
    "            num_gold_q_without_ans += 1\n",
    "            continue\n",
    "        # trim last 4 digits which get removed for the final bioasq form answers\n",
    "        trimmed_id = gold_ids[i][0:20]\n",
    "        if trimmed_id not in gen_factoid_answers.keys():\n",
    "            if DEBUG:\n",
    "                print(f\"{trimmed_id} wasn't correctly identified as factoid\")\n",
    "            continue\n",
    "        gen_vals = gen_factoid_answers[trimmed_id]\n",
    "        gen_vals_clean = [e.lower().strip() for e in gen_vals]\n",
    "        if DEBUG:\n",
    "            print(gold_val, \" | \", gen_vals)\n",
    "        # accuracy calculations\n",
    "        gold_val_clean = gold_val\n",
    "        num_total += 1\n",
    "        if (\n",
    "            gold_val_clean == gen_vals_clean[0]\n",
    "        ):  # force lowercase / strip whitespace to help\n",
    "            num_strict += 1\n",
    "            num_leniant += 1\n",
    "        elif gold_val_clean in gen_vals_clean:\n",
    "            num_leniant += 1\n",
    "\n",
    "        # mrr calculations\n",
    "        mrr = 0\n",
    "        r = 0\n",
    "        n = len(gen_vals_clean)\n",
    "        for i in range(1, n + 1):\n",
    "            if gen_vals_clean[i - 1] == gold_val_clean:\n",
    "                r = i\n",
    "                break\n",
    "        if r != 0:\n",
    "            mrr = 1 / n * 1 / r\n",
    "            if DEBUG:\n",
    "                print(f\"{trimmed_id} MRR: {mrr}\")\n",
    "        mrrs.append(mrr)\n",
    "\n",
    "    average_mrr = sum(mrrs) / len(mrrs)\n",
    "    leniant_acc = num_leniant / num_total\n",
    "    strict_acc = num_strict / num_total\n",
    "\n",
    "    # sanity check\n",
    "    print(\n",
    "        f\"{GREEN}[{len(gold_ids) - num_gold_q_without_ans}/{len(gold_ids)}]{OFF} Factoid questions have answers in gold dataset\"\n",
    "    )\n",
    "    print(\n",
    "        f\"{GREEN}[{num_total}/{len(gen_factoid_answers)}]{OFF} Factoid questions have answers in generated dataset\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Leniant Accuracy: {GREEN}{leniant_acc}{OFF}, Strict Accuracy: {GREEN}{strict_acc}{OFF}, Mean Reciprocal Rank (MRR): {GREEN}{average_mrr}{OFF}\"\n",
    "    )\n",
    "    print(\"\\n\")\n",
    "    return leniant_acc, strict_acc, average_mrr, mrrs\n",
    "\n",
    "\n",
    "def do_list_eval(gold_df, gen_df):\n",
    "    print(f\"{CYAN}List Evaluation{OFF}\")\n",
    "    list_gold_df = gold_df[gold_df[\"type\"] == \"list\"]\n",
    "    list_gen_df = gen_df[gen_df[\"type\"] == \"list\"]\n",
    "\n",
    "    if DEBUG:\n",
    "        print(f\" [{len(list_gold_df)}] Gold List Questions\")\n",
    "        print(f\" [{len(list_gen_df)}] Generated List Questions\")\n",
    "\n",
    "    gold_ids, gold_ans, gen_ids, gen_ans = get_col_list(\n",
    "        list_gold_df, gen_df, \"exact_answer\"\n",
    "    )\n",
    "    num_gen_q_without_ans = 0\n",
    "    num_gold_q_without_ans = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    num_gen = 0\n",
    "    scores = []\n",
    "    # for each question\n",
    "    for i in range(len(gold_ids)):\n",
    "        gold_val = gold_ans[i]\n",
    "        if gold_val == []:\n",
    "            num_gold_q_without_ans += 1\n",
    "            continue\n",
    "        try:\n",
    "            gen_val = gen_ans[gen_ids.index(gold_ids[i])]\n",
    "        except ValueError as e:\n",
    "            if DEBUG:\n",
    "                print(e)\n",
    "            continue\n",
    "        # if answers are found\n",
    "        if gen_val != None:  # List is only able to find a single item list\n",
    "            # TP is answer in Gold AND Gen\n",
    "            # FP is answer NOT IN GOLD, but YES IN GEN\n",
    "            # FN is answer IN Gold but NOT GEN\n",
    "            gold_list = gold_val[0]\n",
    "            for val in gold_list:\n",
    "                if val in gen_val:\n",
    "                    tp += 1\n",
    "                elif val not in gen_val:\n",
    "                    fn += 1\n",
    "            for val in gen_val:\n",
    "                if val not in gold_list:\n",
    "                    fp += 1\n",
    "            num_gen += 1\n",
    "            f1, p, r = get_f1_p_r(tp, fp, fn, tag=\"List Questions\")\n",
    "            scores.append((f1, p, r))\n",
    "    # sanity check\n",
    "    print(\n",
    "        f\"{GREEN}[{len(gold_ids) - num_gold_q_without_ans}/{len(gold_ids)}]{OFF} List questions have answers in gold dataset\"\n",
    "    )\n",
    "    print(\n",
    "        f\"{GREEN}[{num_gen}/{len(list_gen_df)}]{OFF} List questions have answers in generated dataset\"\n",
    "    )\n",
    "\n",
    "    # OVERALL SCORES\n",
    "    f1_sum = p_sum = r_sum = 0\n",
    "    for f1, p, r in scores:\n",
    "        f1_sum += f1\n",
    "        p_sum += p\n",
    "        r_sum += r\n",
    "    f1_sum /= len(scores)\n",
    "    p_sum /= len(scores)\n",
    "    r_sum /= len(scores)\n",
    "\n",
    "    print(\n",
    "        f\"List Questions mean f1 {GREEN}{f1_sum}{OFF}, precision {GREEN}{p_sum}{OFF}, recall {GREEN}{r_sum}{OFF}\"\n",
    "    )\n",
    "    print(\"\\n\")\n",
    "    return f1_sum, p_sum, r_sum, scores\n",
    "\n",
    "\n",
    "def gen_gold_qu_output(gold_df, gen_folder, gen_xml_name=\"bioasq_qa.xml\"):\n",
    "    qu_generated = gen_folder + \"/ir/input/\" + gen_xml_name\n",
    "    new_file_name = qu_generated.replace(\".xml\", \"_GOLD.xml\")\n",
    "\n",
    "    fileTree = et.parse(qu_generated)\n",
    "    if fileTree:\n",
    "        root = fileTree.getroot()\n",
    "        questions = root.findall(\"Q\")\n",
    "        for question in questions:\n",
    "            id = question.attrib.get(\"id\")\n",
    "            qp = question.find(\"QP\")\n",
    "            # remove type and entities\n",
    "            qp.clear()\n",
    "            # type is the fifth element in the row\n",
    "            gold_type = gold_df.loc[gold_df[\"id\"] == id].values[0][4]\n",
    "            type_ele = et.SubElement(qp, \"Type\")\n",
    "            type_ele.text = gold_type\n",
    "            gold_concepts = gold_df.loc[gold_df[\"id\"] == id].values[0][7]\n",
    "            ent_list = []\n",
    "            qp_query = et.SubElement(qp, \"Query\")\n",
    "            if not isinstance(gold_concepts, list):\n",
    "                if DEBUG:\n",
    "                    print(\n",
    "                        f\"Question [{id}] has no golden human_concepts [{type(gold_concepts)}]\"\n",
    "                    )\n",
    "                continue\n",
    "            for ent in gold_concepts:\n",
    "                ent_list.append(str(ent))\n",
    "                qp_en = et.SubElement(qp, \"Entities\")\n",
    "                qp_en.text = str(ent)\n",
    "            qp_query.text = str(\" \".join(ent_list))\n",
    "        tree = et.ElementTree(root)\n",
    "        os.makedirs(os.path.dirname(new_file_name), exist_ok=True)\n",
    "        print(f\"Writing gold QU output / IR input to {new_file_name}\")\n",
    "        tree.write(new_file_name, pretty_print=True)\n",
    "    return new_file_name\n",
    "\n",
    "\n",
    "def gen_gold_ir_output(gold_df, gen_folder, gen_xml_name=\"bioasq_qa.xml\"):\n",
    "    ir_generated = gen_folder + \"/ir/output/\" + gen_xml_name\n",
    "    new_file_name = ir_generated.replace(\".xml\", \"_GOLD.xml\")\n",
    "\n",
    "    fileTree = et.parse(ir_generated)\n",
    "    if fileTree:\n",
    "        root = fileTree.getroot()\n",
    "        questions = root.findall(\"Q\")\n",
    "        for question in questions:\n",
    "            id = question.attrib.get(\"id\")\n",
    "            original_question = question.text\n",
    "            if DEBUG:\n",
    "                print(original_question)\n",
    "            ir = question.find(\"IR\")\n",
    "            # remove original generated articles\n",
    "            ir.clear()\n",
    "\n",
    "            gold_abstracts = gold_df.loc[gold_df[\"id\"] == id].values[0][8]\n",
    "            gold_titles = gold_df.loc[gold_df[\"id\"] == id].values[0][9]\n",
    "            # system just using top abstract atm\n",
    "            if isinstance(gold_abstracts, list):\n",
    "                gold_abstract = gold_abstracts[0]\n",
    "            else:\n",
    "                gold_abstract = \"\"\n",
    "            if isinstance(gold_titles, list):\n",
    "                gold_title = gold_titles[0]\n",
    "            else:\n",
    "                gold_title = \"\"\n",
    "            # fill result\n",
    "            result_tag = et.SubElement(ir, \"Result\")\n",
    "            pmid = gold_df.loc[gold_df['id'] == id].values[0][1][0]\n",
    "            result_tag.set(\"PMID\", pmid)\n",
    "            title = et.SubElement(result_tag, \"Title\")\n",
    "            title.text = gold_title\n",
    "            abstract = et.SubElement(result_tag, \"Abstract\")\n",
    "            abstract.text = gold_abstract\n",
    "        tree = et.ElementTree(root)\n",
    "        os.makedirs(os.path.dirname(new_file_name), exist_ok=True)\n",
    "        print(f\"Writing gold QA input / IR output to {new_file_name}\")\n",
    "        tree.write(new_file_name, pretty_print=True)\n",
    "    return new_file_name\n",
    "\n",
    "\n",
    "def generate_intermediary_datasets(gold_df, qu_output, ir_output):\n",
    "    # QU: replace concepts with human_concepts and type with gold type\n",
    "    gold_qu_output = gen_gold_qu_output(gold_df, qu_output)\n",
    "    # IR: Replace IR with full_abstract in document format\n",
    "    gold_ir_output = gen_gold_ir_output(gold_df, ir_output)\n",
    "    return gold_qu_output, gold_ir_output\n",
    "\n",
    "\n",
    "def get_gold_df(gold_dataset_path):\n",
    "    with open(gold_dataset_path, \"r\") as f:\n",
    "        gold_data = json.loads(f.read())\n",
    "    # load and flatten data\n",
    "    gold_df = pd.json_normalize(gold_data, record_path=\"questions\")\n",
    "    # get gold df\n",
    "    gold_df[\"documents\"] = gold_df[\"documents\"].apply(get_pmid)\n",
    "    return gold_df\n",
    "\n",
    "\n",
    "def run_all_the_tests(gold_dataset_path, generation_folder_path, gen_xml_name):\n",
    "    factoid_path = generation_folder_path + \"/qa/factoid/BioASQform_BioASQ-answer.json\"\n",
    "    generated_qu = generation_folder_path + \"/ir/output/\" + gen_xml_name\n",
    "\n",
    "    with open(gold_dataset_path, \"r\") as f:\n",
    "        gold_data = json.loads(f.read())\n",
    "    # load and flatten data\n",
    "    gold_df = pd.json_normalize(gold_data, record_path=\"questions\")\n",
    "    # get gold df\n",
    "    gold_df[\"documents\"] = gold_df[\"documents\"].apply(get_pmid)\n",
    "    # get generated df\n",
    "    gen_df = parse_xml(generated_qu, generation_folder_path + \"/qa\")\n",
    "\n",
    "    # do tests\n",
    "    concepts_report = do_concepts_eval(gold_df, gen_df)\n",
    "    pmids_report = do_pmids_eval(gold_df, gen_df)\n",
    "    gold_type = gold_df[\"type\"].to_numpy()\n",
    "    gen_type = gen_df[\"type\"].to_numpy()\n",
    "    try:\n",
    "        print(f\"{CYAN}Type Evaluation{OFF}\")\n",
    "        print(classification_report(gold_type, gen_type))\n",
    "        type_report = classification_report(gold_type, gen_type, output_dict=True)\n",
    "    except:\n",
    "        type_report = f\"TypeReport: Found input variables with inconsistent numbers of samples: [{len(gold_type)}] [{len(gen_type)}]\"\n",
    "    yes_no_report = do_yes_no_eval(gold_df, gen_df)\n",
    "    factoid_report = do_factoid_eval(gold_df, gen_df, factoid_path)\n",
    "    list_report = do_list_eval(gold_df, gen_df)\n",
    "\n",
    "    test_results = (\n",
    "        concepts_report,\n",
    "        pmids_report,\n",
    "        type_report,\n",
    "        yes_no_report,\n",
    "        factoid_report,\n",
    "        list_report,\n",
    "    )\n",
    "    save_results(test_results, generation_folder_path)\n",
    "    return test_results\n",
    "\n",
    "\n",
    "def save_results(test_results, save_path):\n",
    "    t = time.localtime()\n",
    "    date = time.strftime(\"%b-%d-%Y\", t)\n",
    "    (\n",
    "        concepts_report,\n",
    "        pmids_report,\n",
    "        type_report,\n",
    "        yes_no_report,\n",
    "        factoid_report,\n",
    "        list_report,\n",
    "    ) = test_results\n",
    "    results_folder = save_path + \"/test_results/\" + date\n",
    "    timestamp = time.strftime(\"%H%M%S\", t)\n",
    "    if not os.path.exists(results_folder):\n",
    "        os.makedirs(results_folder)\n",
    "\n",
    "    # Save concepts\n",
    "    con_name = results_folder + f\"/concepts-{timestamp}.csv\"\n",
    "    with open(con_name, \"w+\") as f:\n",
    "        f1_sum, p_sum, r_sum, scores = concepts_report\n",
    "        f.write(\"Average f1 score,Average precision,Average Recall\\n\")\n",
    "        f.write(f\"{f1_sum},{p_sum},{r_sum}\\n\")\n",
    "        f.write(\"f1,precision,recall\\n\")\n",
    "        for f1, p, r in scores:\n",
    "            f.write(f\"{f1},{p},{r}\\n\")\n",
    "    print(f\"Saved concepts info to {con_name}\")\n",
    "\n",
    "    # Save pmids\n",
    "    pmid_name = results_folder + f\"/pmids-{timestamp}.csv\"\n",
    "    with open(pmid_name, \"w+\") as f:\n",
    "        f1_sum, p_sum, r_sum, scores = pmids_report\n",
    "        f.write(\"Average f1 score,Average precision,Average Recall\\n\")\n",
    "        f.write(f\"{f1_sum},{p_sum},{r_sum}\\n\")\n",
    "        f.write(\"f1,precision,recall\\n\")\n",
    "        for f1, p, r in scores:\n",
    "            f.write(f\"{f1},{p},{r}\\n\")\n",
    "    print(f\"Saved pmid info to {pmid_name}\")\n",
    "\n",
    "    # Save type report\n",
    "    type_name = results_folder + f\"/type-{timestamp}.json\"\n",
    "    with open(type_name, \"w+\") as f:\n",
    "        json.dump(type_report, f, indent=2)\n",
    "    print(f\"Saved type info to {type_name}\")\n",
    "\n",
    "    # Save yes/no\n",
    "    yesno_name = results_folder + f\"/yesno-{timestamp}.csv\"\n",
    "    with open(yesno_name, \"w+\") as f:\n",
    "        yf1, yp, yr, nf1, np, nr, f1, p, r = yes_no_report\n",
    "        f.write(\"Yes/No f1 score,Yes/No precision ,Yes/No recall\\n\")\n",
    "        f.write(f\"{f1},{p},{r}\\n\")\n",
    "        f.write(\"Yes f1 score,Yes precision ,Yes recall\\n\")\n",
    "        f.write(f\"{yf1},{yp},{yr}\\n\")\n",
    "        f.write(\"No f1 score,No precision ,No recall\\n\")\n",
    "        f.write(f\"{nf1},{np},{nr}\\n\")\n",
    "    print(f\"Saved yes/no info to {yesno_name}\")\n",
    "\n",
    "    # Save factoids\n",
    "    fact_name = results_folder + f\"/factoid-{timestamp}.csv\"\n",
    "    with open(fact_name, \"w+\") as f:\n",
    "        leniant_acc, strict_acc, average_mrr, mrrs = factoid_report\n",
    "        f.write(\n",
    "            \"Factoid leniant accuracy,Factoid strict accuracy,Factoid average mrr\\n\"\n",
    "        )\n",
    "        f.write(f\"{leniant_acc},{strict_acc},{average_mrr}\\n\")\n",
    "        f.write(\"mrr\\n\")\n",
    "        for mrr in mrrs:\n",
    "            f.write(f\"{mrr}\\n\")\n",
    "    print(f\"Saved factoid info to {fact_name}\")\n",
    "\n",
    "    # Save list\n",
    "    list_name = results_folder + f\"/list-{timestamp}.csv\"\n",
    "    with open(list_name, \"w+\") as f:\n",
    "        f1_sum, p_sum, r_sum, scores = list_report\n",
    "        f.write(\"Average f1 score,Average precision,Average Recall\\n\")\n",
    "        f.write(f\"{f1_sum},{p_sum},{r_sum}\\n\")\n",
    "        f.write(\"f1,precision,recall\\n\")\n",
    "        for f1, p, r in scores:\n",
    "            f.write(f\"{f1},{p},{r}\\n\")\n",
    "    print(f\"Saved list info to {list_name}\")\n",
    "\n",
    "\n",
    "# Set up the golden answer dataframe\n",
    "\"\"\"\n",
    "golden_dataset_path = \"testing_datasets/augmented_concepts_abstracts_titles.json\"\n",
    "gen_folder = \"tmp\"\n",
    "xml_name = \"bioasq_qa.xml\"\n",
    "\n",
    "test_results = run_all_the_tests(golden_dataset_path, gen_folder, xml_name)\n",
    "save_results(test_results, gen_folder)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3243 entries, 0 to 3242\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   body            3243 non-null   object\n",
      " 1   documents       3243 non-null   object\n",
      " 2   ideal_answer    3243 non-null   object\n",
      " 3   concepts        1854 non-null   object\n",
      " 4   type            3243 non-null   object\n",
      " 5   id              3243 non-null   object\n",
      " 6   snippets        3243 non-null   object\n",
      " 7   human_concepts  1854 non-null   object\n",
      " 8   full_abstracts  3243 non-null   object\n",
      " 9   titles          3243 non-null   object\n",
      " 10  triples         322 non-null    object\n",
      " 11  exact_answer    2466 non-null   object\n",
      "dtypes: object(12)\n",
      "memory usage: 304.2+ KB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run system raw\n",
    "golden_dataset_path = \"testing_datasets/augmented_concepts_abstracts_titles.json\"\n",
    "gen_folder = \"tmp\"\n",
    "xml_name = \"bioasq_qa.xml\"\n",
    "gold_df =  get_gold_df(golden_dataset_path)\n",
    "gold_df.info()\n",
    "#gold_ir_output  = gen_gold_qu_output(gold_df,gen_folder,xml_name)\n",
    "# gen_gold_ir_output(gold_df,gen_folder,xml_name)\n",
    "# xml_name = \"bioasq_qa_SMALL_GOLD.xml\"\n",
    "# gold_qu_ir_test_results = run_all_the_tests(golden_dataset_path, gen_folder,xml_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Acrokeratosis paraneoplastica of Bazex is a rare but important paraneoplastic dermatosis, usually manifesting as psoriasiform rashes over the acral sites.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare by getting the top snippets instead of abstracts.\n",
    "\n",
    "# instead of using full_abstracts, use the top snippets for this \n",
    "id  = \"56bc751eac7ad10019000013\"\n",
    "gold_df.loc[gold_df[\"id\"] == id].values[0][6][0].get(\"text\")\n",
    "\n",
    "# iterate through [0][6][n] number of snippets for gold snippets\n",
    "\n",
    "def gen_gold_ir_output_FROM_SNIPPETS(gold_df, gen_folder, gen_xml_name=\"bioasq_qa.xml\"):\n",
    "    ir_generated = gen_folder + \"/ir/output/\" + gen_xml_name\n",
    "    new_file_name = ir_generated.replace(\".xml\", \"_GOLD.xml\")\n",
    "\n",
    "    fileTree = et.parse(ir_generated)\n",
    "    if fileTree:\n",
    "        root = fileTree.getroot()\n",
    "        questions = root.findall(\"Q\")\n",
    "        for question in questions:\n",
    "            id = question.attrib.get(\"id\")\n",
    "            original_question = question.text\n",
    "            if DEBUG:\n",
    "                print(original_question)\n",
    "            ir = question.find(\"IR\")\n",
    "            # remove original generated articles\n",
    "            ir.clear()\n",
    "            gold_snippet =  gold_df.loc[gold_df[\"id\"] == id].values[0][6][0].get(\"text\")\n",
    "            # gold_abstracts = gold_df.loc[gold_df[\"id\"] == id].values[0][8]\n",
    "            gold_titles = gold_df.loc[gold_df[\"id\"] == id].values[0][9]\n",
    "            # system just using top abstract atm\n",
    "            # if isinstance(gold_abstracts, list):\n",
    "            #     gold_abstract = gold_abstracts[0]\n",
    "            # else:\n",
    "            #     gold_abstract = \"\"\n",
    "            if isinstance(gold_titles, list):\n",
    "                gold_title = gold_titles[0]\n",
    "            else:\n",
    "                gold_title = \"\"\n",
    "            # fill result\n",
    "            result_tag = et.SubElement(ir, \"Result\")\n",
    "            pmid = gold_df.loc[gold_df['id'] == id].values[0][1][0]\n",
    "            result_tag.set(\"PMID\", pmid)\n",
    "            title = et.SubElement(result_tag, \"Title\")\n",
    "            title.text = gold_title\n",
    "            abstract = et.SubElement(result_tag, \"Abstract\")\n",
    "            abstract.text = gold_snippet # pass in the snippet instead of the abstract for those with abstracts\n",
    "        tree = et.ElementTree(root)\n",
    "        os.makedirs(os.path.dirname(new_file_name), exist_ok=True)\n",
    "        print(f\"Writing gold QA input / IR output to {new_file_name}\")\n",
    "        tree.write(new_file_name, pretty_print=True)\n",
    "    return new_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "80370dbcfc3023cdbcb71d202b3d45870b2ec245031d0411f339cfb8d50c0055"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('3.7.9': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
