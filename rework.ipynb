{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree as et\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "\n",
    "from utils import *\n",
    "\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pmid(docs):\n",
    "    documents = [\n",
    "        document.split(\"/\")[-1] for document in docs\n",
    "    ]\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers(answers_files):\n",
    "    all_answers = dict()\n",
    "    for a in answers_files:\n",
    "        with open(a, \"r\") as f:\n",
    "            d = json.loads(f.read())\n",
    "            if DEBUG:\n",
    "                print(f\"{len(d)} answers found in {a}\")\n",
    "            for key, value in d.items():\n",
    "                if key in all_answers.keys():\n",
    "                    print(f\"MULTIPLE ANSWERS FOR {key}\")\n",
    "                if isinstance(value,list):\n",
    "                    all_answers[key] = value[0] # get the first value which is answer not prediction for the yes/no\n",
    "                else:\n",
    "                    all_answers[key] = value\n",
    "    return all_answers\n",
    "\n",
    "def get_three_files(a_dir):\n",
    "    return [\n",
    "        a_dir + \"/factoid/predictions.json\",\n",
    "        a_dir + \"/list/predictions.json\",\n",
    "        a_dir + \"/yesno/predictions.json\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_xml(xml_file, dir_for_qa): \n",
    "    no_answers = 0\n",
    "    # get answers\n",
    "    qa_answers = get_answers(get_three_files(dir_for_qa))\n",
    "    # get ir and qu\n",
    "    df_cols = ['id','human_concepts','documents','full_abstracts','titles','type', 'exact_answer']\n",
    "    xtree = et.parse(xml_file)\n",
    "    xroot = xtree.getroot()\n",
    "    rows = []\n",
    "    for question in xroot: \n",
    "        id = question.attrib.get(\"id\")\n",
    "        ir = question.find(\"IR\")\n",
    "        qp = question.find(\"QP\")\n",
    "        concepts = [e.text for e in qp.findall(\"Entities\")]\n",
    "        qa_type = qp.find(\"Type\").text\n",
    "        titles =  [e.find(\"Title\").text for e in ir.findall(\"Result\")]\n",
    "        abstracts =  [e.find(\"Abstract\").text for e in ir.findall(\"Result\")]\n",
    "        pmids = [e.get(\"PMID\") for e in ir.findall(\"Result\")]\n",
    "        exact_answer = qa_answers[id] if id in qa_answers else None\n",
    "        if DEBUG and not exact_answer:\n",
    "            print(f\"id [{id}] has no answer\")\n",
    "            no_answers +=1\n",
    "        rows.append({\"id\":id,\"human_concepts\":concepts,\"documents\":pmids,\"full_abstracts\":abstracts,\"titles\":titles,\"type\":qa_type,'exact_answer':exact_answer})\n",
    "    out_df = pd.DataFrame(rows, columns=df_cols)\n",
    "    if DEBUG:\n",
    "        print(f\"[{no_answers}/{len(out_df)}] questions had answers\")\n",
    "    return out_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the golden answer dataframe\n",
    "golden_dataset_path = \"testing_datasets/augmented_concepts_abstracts_titles.json\"\n",
    "generated_qu = \"tmp/ir/output/bioasq_qa.xml\"\n",
    "with open(golden_dataset_path,'r') as f:\n",
    "    gold_data = json.loads(f.read())\n",
    "# load and flatten data\n",
    "gold_df = pd.json_normalize(gold_data,record_path=\"questions\")\n",
    "# get gold df\n",
    "gold_df['documents'] = gold_df['documents'].apply(get_pmid)\n",
    "# get generated df\n",
    "gen_df = parse_xml(generated_qu,'tmp/qa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes/No Evaluation\n",
      "Gold\n",
      " [881] Gold Yes/No Questions\n",
      " [704] Gold Yes Questions\n",
      " [177] Gold No Questions\n",
      "Generated\n",
      " [909] Generated Yes/No Questions\n",
      " [0] Generated Yes Questions\n",
      " [523] Generated No Questions\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "def print_yes_no_info(df, tag):\n",
    "    print(tag)\n",
    "    print(f\" [{len(df)}] {tag} Yes/No Questions\")\n",
    "    yes_df = df[df['exact_answer'] == 'yes']\n",
    "    no_df = df[df['exact_answer'] == 'no']\n",
    "    print(f\" [{len(yes_df)}] {tag} Yes Questions\")\n",
    "    print(f\" [{len(no_df)}] {tag} No Questions\")\n",
    "\n",
    "\n",
    "\"\"\" f1 Yes\n",
    "    tp is yes in both\n",
    "    fp is number of 'yes' in generated when gold is 'no'\n",
    "    fn is number of 'no' in generated when gold is 'yes'\n",
    "\n",
    "    f1 No\n",
    "    tp is No in both\n",
    "    fp is number of 'no' in generated when gold is 'yes'\n",
    "    fn is number of 'yes' in generated when gold is 'no'\n",
    "\n",
    "    IGNORE if the predicted type is yes/no but gold type is different\n",
    "\"\"\"\n",
    "def do_yes_no_eval(gold_df,gen_df):\n",
    "    print(\"Yes/No Evaluation\")\n",
    "    yes_no_gold_df = gold_df[gold_df['type'] == 'yesno']\n",
    "    yes_no_gen_df = gen_df[gen_df['type'] == 'yesno']\n",
    "    # Gold stats\n",
    "    print_yes_no_info(yes_no_gold_df, \"Gold\")\n",
    "    # Gen Stats\n",
    "    print_yes_no_info(yes_no_gen_df, \"Generated\")\n",
    "\n",
    "    gold_answers = yes_no_gold_df.loc[:,['id','exact_answer']].copy()\n",
    "    gen_answers = gen_df.loc[:,['id','exact_answer']].copy() # grabbing all because there could be incorrect type guesses\n",
    "    \n",
    "    # YES SIDE\n",
    "    gold = gold_answers.to_dict(orient='list')\n",
    "    gen = gen_answers.to_dict(orient='list')\n",
    "    print(len(gold))\n",
    "\n",
    "    # NO SIDE\n",
    "    return \"\"\n",
    "\n",
    "yes_no_results = do_yes_no_eval(gold_df,gen_df)\n",
    "#print(yes_no_results)\n",
    "\n",
    "\n",
    "# so if the gold_gen[id].exact_answer == gen[exact_answer], tp ++ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report for type evaluation step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     factoid       0.93      0.84      0.88       941\n",
      "        list       0.94      0.95      0.95       644\n",
      "     summary       0.84      0.91      0.87       777\n",
      "       yesno       0.97      1.00      0.98       881\n",
      "\n",
      "    accuracy                           0.92      3243\n",
      "   macro avg       0.92      0.92      0.92      3243\n",
      "weighted avg       0.92      0.92      0.92      3243\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get classification reports\n",
    "print(\"Report for type evaluation step\")\n",
    "type_report = classification_report(gold_df['type'].to_numpy(),gen_df['type'].to_numpy(),output_dict=DEBUG)\n",
    "print(type_report)\n",
    "\n",
    "# yes_no_results = do_yes_no_eval(gold_df,gen_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "yesn_no_gold_df = gold_df[gold_df['type'] == 'yesno']\n",
    "factoid_gold_df = gold_df[gold_df['type'] == 'factoid']\n",
    "list_gold_df = gold_df[gold_df['type'] == 'list']\n",
    "\n",
    "yesn_no_gen_df = gen_df[gen_df['type'] == 'yesno']\n",
    "factoid_gen_df = gen_df[gen_df['type'] == 'factoid']\n",
    "list_gen_df = gen_df[gen_df['type'] == 'list']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "80370dbcfc3023cdbcb71d202b3d45870b2ec245031d0411f339cfb8d50c0055"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('3.7.9': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
