{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree as et\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "\n",
    "from utils import *\n",
    "\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pmid(docs):\n",
    "    documents = [\n",
    "        document.split(\"/\")[-1] for document in docs\n",
    "    ]\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers(answers_files):\n",
    "    all_answers = dict()\n",
    "    for a in answers_files:\n",
    "        with open(a, \"r\") as f:\n",
    "            d = json.loads(f.read())\n",
    "            if DEBUG:\n",
    "                print(f\"{len(d)} answers found in {a}\")\n",
    "            for key, value in d.items():\n",
    "                if key in all_answers.keys():\n",
    "                    print(f\"MULTIPLE ANSWERS FOR {key}\")\n",
    "                if isinstance(value,list):\n",
    "                    all_answers[key] = value[0] # get the first value which is answer not prediction for the yes/no\n",
    "                else:\n",
    "                    all_answers[key] = value\n",
    "    return all_answers\n",
    "\n",
    "def get_three_files(a_dir):\n",
    "    return [\n",
    "        a_dir + \"/factoid/predictions.json\",\n",
    "        a_dir + \"/list/predictions.json\",\n",
    "        a_dir + \"/yesno/predictions.json\",\n",
    "    ]\n",
    "\n",
    "def get_col_list(gold_df,gen_df,col):\n",
    "    gold_col = gold_df.loc[:,['id',col]].copy()\n",
    "    gen_col = gen_df.loc[:,['id',col]].copy() \n",
    "    \n",
    "    gold = gold_col.to_dict(orient='list')\n",
    "    gen = gen_col.to_dict(orient='list')\n",
    "    gen_ids = gen['id']\n",
    "    gen_vals = gen[col]\n",
    "    gold_ids = gold['id']\n",
    "    gold_vals = gold[col]\n",
    "    return gold_ids,gold_vals,gen_ids,gen_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_xml(xml_file, dir_for_qa): \n",
    "    no_answers = 0\n",
    "    # get answers\n",
    "    qa_answers = get_answers(get_three_files(dir_for_qa))\n",
    "    # get ir and qu\n",
    "    df_cols = ['id','human_concepts','documents','full_abstracts','titles','type', 'exact_answer']\n",
    "    xtree = et.parse(xml_file)\n",
    "    xroot = xtree.getroot()\n",
    "    rows = []\n",
    "    for question in xroot: \n",
    "        id = question.attrib.get(\"id\")\n",
    "        ir = question.find(\"IR\")\n",
    "        qp = question.find(\"QP\")\n",
    "        concepts = [e.text for e in qp.findall(\"Entities\")]\n",
    "        qa_type = qp.find(\"Type\").text\n",
    "        titles =  [e.find(\"Title\").text for e in ir.findall(\"Result\")]\n",
    "        abstracts =  [e.find(\"Abstract\").text for e in ir.findall(\"Result\")]\n",
    "        pmids = [e.get(\"PMID\") for e in ir.findall(\"Result\")]\n",
    "        exact_answer = qa_answers[id] if id in qa_answers else None\n",
    "        if DEBUG and not exact_answer:\n",
    "            print(f\"id [{id}] has no answer\")\n",
    "            no_answers +=1\n",
    "        rows.append({\"id\":id,\"human_concepts\":concepts,\"documents\":pmids,\"full_abstracts\":abstracts,\"titles\":titles,\"type\":qa_type,'exact_answer':exact_answer})\n",
    "    out_df = pd.DataFrame(rows, columns=df_cols)\n",
    "    if DEBUG:\n",
    "        print(f\"{GREEN}[{no_answers}/{len(out_df)}]{OFF} {WHITE}questions had answers{OFF}\")\n",
    "    return out_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_yes_no_info(df, tag):\n",
    "    print(tag)\n",
    "    print(f\" [{len(df)}] {tag} Yes/No Questions\")\n",
    "    yes_df = df[df['exact_answer'] == 'yes']\n",
    "    no_df = df[df['exact_answer'] == 'no']\n",
    "    print(f\" [{len(yes_df)}] {tag} Yes Questions\")\n",
    "    print(f\" [{len(no_df)}] {tag} No Questions\")\n",
    "\n",
    "\n",
    "\"\"\" f1 Yes\n",
    "    tp is gen 'yes' | gold 'yes'\n",
    "    fp is gen 'yes' | gold 'no'\n",
    "    fn is gen 'no' |  gold 'yes'\n",
    "\n",
    "    f1 No\n",
    "    tp is gen 'no' | gold 'no'\n",
    "    fp is gen 'no' | gold 'yes'\n",
    "    fn is gen 'yes' |  gold 'no'\n",
    "\n",
    "    IGNORE if the predicted type is yes/no but gold type is different\n",
    "\"\"\"\n",
    "def do_yes_no_eval(gold_df,gen_df):\n",
    "    print(f\"{CYAN}Yes/No Evaluation{OFF}\")\n",
    "    yes_no_gold_df = gold_df[gold_df['type'] == 'yesno']\n",
    "    yes_no_gen_df = gen_df[gen_df['type'] == 'yesno']\n",
    "    \n",
    "    if DEBUG:\n",
    "        # Gold stats\n",
    "        print_yes_no_info(yes_no_gold_df, \"Gold\")\n",
    "        # Gen Stats\n",
    "        print_yes_no_info(yes_no_gen_df, \"Generated\")\n",
    "\n",
    "    gold_ids,gold_ans,gen_ids,gen_ans = get_col_list(yes_no_gold_df,gen_df,'exact_answer')\n",
    "\n",
    "    # YES \n",
    "    ytp = 0\n",
    "    yfp = 0\n",
    "    yfn = 0\n",
    "\n",
    "    for i in range (len(gold_ids)):\n",
    "        gold_val = gold_ans[i]\n",
    "        gen_val = gen_ans[gen_ids.index(gold_ids[i])]\n",
    "        if gen_val:\n",
    "            if gold_val == 'yes':\n",
    "                if gen_val =='yes':\n",
    "                    ytp += 1\n",
    "                elif gen_val =='no':\n",
    "                    yfn += 1\n",
    "                else:\n",
    "                    if DEBUG:\n",
    "                        print(f\"yes question [{gold_ids[i]}] had generated answer {gen_val}\")\n",
    "            elif gold_val == 'no':\n",
    "                if gen_val == 'yes':\n",
    "                    yfp +=1\n",
    "                elif gen_val =='no':\n",
    "                    pass #handled by no f1\n",
    "                else:\n",
    "                    if DEBUG:\n",
    "                        print(f\"no question [{gold_ids[i]}] had generated answer {gen_val}\")\n",
    "            else:\n",
    "                print(f\"GOLDEN answer to yes/no question [{gold_ids[i]}] was {gold_val}\")\n",
    "                \n",
    "        else: # not identified as yes/no question by generated\n",
    "            pass\n",
    "    #sanity check\n",
    "    print (f\"Yes | True Posative: {GREEN}{ytp}{OFF}, False Posative: {GREEN}{yfp}{OFF}, False Negative: {GREEN}{yfn}{OFF}\")\n",
    "    try:\n",
    "        yp = ytp/(ytp + yfp)\n",
    "    except:\n",
    "        yp = 0\n",
    "    try:\n",
    "        yr = ytp/(ytp + yfn)\n",
    "    except:\n",
    "        yr = 0\n",
    "    try:\n",
    "        yf1 = 2 * ((yp * yr)/(yp+yr))\n",
    "    except:\n",
    "        yf1 = 0\n",
    "    print (f'Yes | f1 {GREEN}{yf1}{OFF}, precision {GREEN}{yp}{OFF}, recall {GREEN}{yr}{OFF}')\n",
    "\n",
    "    # NO SIDE\n",
    "    ntp = 0\n",
    "    nfp = 0\n",
    "    nfn = 0\n",
    "\n",
    "    for i in range (len(gold_ids)):\n",
    "        gold_val = gold_ans[i]\n",
    "        gen_val = gen_ans[gen_ids.index(gold_ids[i])]\n",
    "        if gen_val:\n",
    "            if gold_val == 'no':\n",
    "                if gen_val =='no':\n",
    "                    ntp += 1\n",
    "                elif gen_val =='yes':\n",
    "                    nfn += 1\n",
    "                else:\n",
    "                    if DEBUG:\n",
    "                        print(f\"no question [{gold_ids[i]}] had generated answer {gen_val}\")\n",
    "            elif gold_val == 'yes':\n",
    "                if gen_val == 'no':\n",
    "                    nfp +=1\n",
    "                elif gen_val =='yes':\n",
    "                    pass #handled by no f1\n",
    "                else:\n",
    "                    print(f\"yes question [{gold_ids[i]}] had generated answer {gen_val}\")\n",
    "            else:\n",
    "                if DEBUG:\n",
    "                    print(f\"GOLDEN answer to yes/no question [{gold_ids[i]}] was {gold_val}\")\n",
    "                \n",
    "        else: # not identified as yes/no question by generated\n",
    "            pass\n",
    "    \n",
    "    # sanity check\n",
    "    print (f\"No | True Posative: {GREEN}{ntp}{OFF}, False Posative: {GREEN}{nfp}{OFF}, False Negative: {GREEN}{nfn}{OFF}\")\n",
    "    try:\n",
    "        np = ntp/(ntp + nfp)\n",
    "    except:\n",
    "        np = 0\n",
    "    try:\n",
    "        nr = ntp/(ntp + nfn)\n",
    "    except:\n",
    "        nr = 0\n",
    "    try:\n",
    "        nf1 = 2 * ((np * nr)/(np+nr))\n",
    "    except:\n",
    "        nf1 = 0\n",
    "    print (f'No | f1 {GREEN}{nf1}{OFF}, precision {GREEN}{np}{OFF}, recall {GREEN}{nr}{OFF}')\n",
    "\n",
    "    f1 = (yf1 + nf1)/2 \n",
    "    p = (yp + np)/2 \n",
    "    r = (yr + nr)/2 \n",
    "    print(f\"Overall Yes/No | f1 {GREEN}{f1}{OFF}, precision {GREEN}{p}{OFF}, recall {GREEN}{r}{OFF}\")\n",
    "    print ('\\n')\n",
    "    return yf1,yp,yr,nf1,np,nr,f1\n",
    "\n",
    "# yes_no_report = do_yes_no_eval(gold_df,gen_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute [[Mean average precision, Geometric mean average precision]], precision, recall, f1 score\n",
    "def do_concepts_eval(gold_df,gen_df):\n",
    "    print(f\"{CYAN}Concepts Evaluation{OFF}\")\n",
    "    gold_ids,gold_cons,gen_ids,gen_cons = get_col_list(gold_df,gen_df,'human_concepts')\n",
    "    num_gen_q_without_cons = 0\n",
    "    num_gold_q_without_cons = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    scores = []\n",
    "    # for each question\n",
    "    for i in range (len(gold_ids)):\n",
    "        gold_val = gold_cons[i]\n",
    "        if not isinstance(gold_val,list) or gold_val == []:\n",
    "            num_gold_q_without_cons += 1\n",
    "            continue\n",
    "        gen_val = gen_cons[gen_ids.index(gold_ids[i])]\n",
    "        # if concepts are found\n",
    "        if gen_val != []:\n",
    "            # TP is concept in Gold AND Gen\n",
    "            # FP is concept NOT IN GOLD, but YES IN GEN\n",
    "            # FN is concept IN Gold but NOT GEN\n",
    "\n",
    "            # get unique concepts from both gold and gen\n",
    "            unique_gold_cons = set(gold_val[0])\n",
    "            unique_gen_cons = set(gen_val[0])\n",
    "            for val in unique_gold_cons:\n",
    "                if val in unique_gen_cons:\n",
    "                    tp += 1\n",
    "                elif val not in unique_gen_cons:\n",
    "                    fn += 1\n",
    "            for val in unique_gen_cons:\n",
    "                if val not in unique_gold_cons:\n",
    "                    fp += 1\n",
    "\n",
    "            f1,p,r = get_f1_p_r(tp,fp,fn, tag =\"Concepts\")\n",
    "            scores.append((f1,p,r))\n",
    "        else: # There are no concepts retrieved for this document\n",
    "            num_gen_q_without_cons += 1\n",
    "            pass\n",
    "    #sanity check\n",
    "    print(f\"{GREEN}[{len(gold_ids) - num_gold_q_without_cons}/{len(gold_ids)}]{OFF} Questions have human readable concepts in gold dataset\")\n",
    "    print(f\"{GREEN}[{len(gen_ids) - num_gen_q_without_cons}/{len(gen_ids)}]{OFF} Questions have human readable concepts in generated dataset\")\n",
    "\n",
    "    # OVERALL SCORES\n",
    "    f1_sum = p_sum = r_sum = 0\n",
    "    for f1,p,r in scores:\n",
    "        f1_sum += f1\n",
    "        p_sum += p\n",
    "        r_sum += r\n",
    "    f1_sum /= len(scores)\n",
    "    p_sum /= len(scores)\n",
    "    r_sum /= len(scores)\n",
    "\n",
    "    print (f'Concepts mean f1 {GREEN}{f1_sum}{OFF}, precision {GREEN}{p_sum}{OFF}, recall {GREEN}{r_sum}{OFF}')\n",
    "    print ('\\n')\n",
    "    return f1_sum,p_sum,r_sum,scores\n",
    "\n",
    "# concepts_report = do_concepts_eval(gold_df,gen_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1_p_r(tp,fp,fn, tag =\"calculated\"):\n",
    "    if DEBUG:\n",
    "        print (f\"{tag} tp: {tp}, fp: {fp}, fn: {fn}\")\n",
    "    try:\n",
    "        p = tp/(tp + fp)\n",
    "    except:\n",
    "        p = 0\n",
    "    try:\n",
    "        r = tp/(tp + fn)\n",
    "    except:\n",
    "        r = 0\n",
    "    try:\n",
    "        f1 = 2 * ((p * r)/(p+r))\n",
    "    except:\n",
    "        f1 = 0\n",
    "    if DEBUG:\n",
    "        print (f'{tag} f1 {f1}, precision {p}, recall {r}')\n",
    "    return f1,p,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_pmids_eval(gold_df,gen_df):\n",
    "    print(f\"{CYAN}PubMed Documents Evaluation{OFF}\")\n",
    "    # pmids are the pubmed document ids\n",
    "    gold_ids,gold_pmids,gen_ids,gen_pmids = get_col_list(gold_df,gen_df,'documents')\n",
    "    num_gen_q_without_docs = 0\n",
    "    num_gold_q_without_docs = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    scores = []\n",
    "    # for each question\n",
    "    for i in range (len(gold_ids)):\n",
    "        gold_val = gold_pmids[i]\n",
    "        if gold_val == []:\n",
    "            num_gold_q_without_docs += 1\n",
    "            continue\n",
    "        gen_val = gen_pmids[gen_ids.index(gold_ids[i])]\n",
    "        # if documents are found\n",
    "        if gen_val != []:\n",
    "            # TP is pmid in Gold AND Gen\n",
    "            # FP is pmid NOT IN GOLD, but YES IN GEN\n",
    "            # FN is pmid IN Gold but NOT GEN\n",
    "\n",
    "            # get unique PMIDs from both gold and gen\n",
    "            unique_gold_pmids = set(gold_val[0])\n",
    "            unique_gen_pmids = set(gen_val[0])\n",
    "            for val in unique_gold_pmids:\n",
    "                if val in unique_gen_pmids:\n",
    "                    tp += 1\n",
    "                elif val not in unique_gen_pmids:\n",
    "                    fn += 1\n",
    "            for val in unique_gen_pmids:\n",
    "                if val not in unique_gold_pmids:\n",
    "                    fp += 1\n",
    "\n",
    "            f1,p,r = get_f1_p_r(tp,fp,fn, tag =\"PubMed Documents\")\n",
    "            scores.append((f1,p,r))\n",
    "        else: # There are no documents retrieved for this document\n",
    "            num_gen_q_without_docs += 1\n",
    "            pass\n",
    "    #sanity check\n",
    "    print(f\"{GREEN}[{len(gold_ids) - num_gold_q_without_docs}/{len(gold_ids)}]{OFF} Questions have documents in gold dataset\")\n",
    "    print(f\"{GREEN}[{len(gen_ids) - num_gen_q_without_docs}/{len(gen_ids)}]{OFF} Questions have documents in generated dataset\")\n",
    "\n",
    "    # OVERALL SCORES\n",
    "    f1_sum = p_sum = r_sum = 0\n",
    "    for f1,p,r in scores:\n",
    "        f1_sum += f1\n",
    "        p_sum += p\n",
    "        r_sum += r\n",
    "    f1_sum /= len(scores)\n",
    "    p_sum /= len(scores)\n",
    "    r_sum /= len(scores)\n",
    "\n",
    "    print (f'PubMed Documents mean f1 {GREEN}{f1_sum}{OFF}, precision {GREEN}{p_sum}{OFF}, recall {GREEN}{r_sum}{OFF}')\n",
    "    print ('\\n')\n",
    "    return f1_sum,p_sum,r_sum,scores\n",
    "\n",
    "# pmid_report = do_pmids_eval(gold_df,gen_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use strict and leniant accuracy  (first result, or any result)\n",
    "def do_factoid_eval(gold_df,gen_factoid_path):\n",
    "    print(f\"{CYAN}Factoid Evaluation{OFF}\")\n",
    "    factoid_gold_df = gold_df[gold_df['type'] == 'factoid']\n",
    "    factoid_gen_df = gen_df[gen_df['type'] == 'factoid']\n",
    "\n",
    "    if DEBUG:\n",
    "        print(f\" [{len(factoid_gold_df)}] Gold Factoid Questions\")\n",
    "        print(f\" [{len(factoid_gen_df)}] Generated Factoid Questions\")\n",
    "    gold_ids,gold_ans,gen_ids,gen_ans = get_col_list(factoid_gold_df,gen_df,'exact_answer')\n",
    "\n",
    "    # Use alternative strategy to handle ranked factoid preds\n",
    "    with open(gen_factoid_path, \"r\") as ft_file:\n",
    "        factoid_gen_json = json.load(ft_file)\n",
    "    \n",
    "    gen_factoid_answers = {}\n",
    "    for question in factoid_gen_json[\"questions\"]:\n",
    "        id = question[\"id\"]\n",
    "        if len(id) == 24:\n",
    "            id = id[0:20]\n",
    "        answer = question['exact_answer']\n",
    "        if isinstance(answer,list): \n",
    "            if isinstance(answer[0],list): # handle list in list\n",
    "                answer = [e[0] for e in answer]\n",
    "        gen_factoid_answers[id] = answer\n",
    "\n",
    "    num_gold_q_without_ans = 0\n",
    "    num_strict = 0\n",
    "    num_leniant = 0\n",
    "    num_total = 0\n",
    "    mrrs = []\n",
    "    # for each question\n",
    "    for i in range (len(gold_ids)):\n",
    "        gold_val = gold_ans[i][0]\n",
    "        if gold_val == []:\n",
    "            num_gold_q_without_ans += 1\n",
    "            continue\n",
    "        # trim last 4 digits which get removed for the final bioasq form answers\n",
    "        trimmed_id = gold_ids[i][0:20]\n",
    "        if trimmed_id not in gen_factoid_answers.keys():\n",
    "            if DEBUG:\n",
    "                print(f\"{trimmed_id} wasn't correctly identified as factoid\")\n",
    "            continue\n",
    "        gen_vals = gen_factoid_answers[trimmed_id]\n",
    "        gen_vals_clean = [e.lower().strip() for e in gen_vals]\n",
    "        if DEBUG:\n",
    "            print(gold_val,\" | \",gen_vals)\n",
    "        # accuracy calculations\n",
    "        gold_val_clean = gold_val\n",
    "        num_total += 1\n",
    "        if gold_val_clean == gen_vals_clean[0]: # force lowercase / strip whitespace to help\n",
    "            num_strict += 1\n",
    "            num_leniant += 1\n",
    "        elif gold_val_clean in gen_vals_clean:\n",
    "            num_leniant += 1\n",
    "\n",
    "        # mrr calculations\n",
    "        mrr = 0\n",
    "        r = 0\n",
    "        n = len(gen_vals_clean)\n",
    "        for i in range(1,n+1):\n",
    "            if gen_vals_clean[i-1] == gold_val_clean:\n",
    "                r = i\n",
    "                break\n",
    "        if r != 0:\n",
    "            mrr = 1/n * 1/r \n",
    "            if DEBUG:\n",
    "                print(f\"{trimmed_id} MRR: {mrr}\")\n",
    "        mrrs.append(mrr)\n",
    "\n",
    "    average_mrr = sum(mrrs) / len(mrrs)\n",
    "    leniant_acc = num_leniant/num_total\n",
    "    strict_acc = num_strict/num_total\n",
    "\n",
    "    # sanity check\n",
    "    print(f\"{GREEN}[{len(gold_ids) - num_gold_q_without_ans}/{len(gold_ids)}]{OFF} Factoid questions have answers in gold dataset\")\n",
    "    print(f\"{GREEN}[{num_total}/{len(gen_factoid_answers)}]{OFF} Factoid questions have answers in generated dataset\")\n",
    "    print(f\"Leniant Accuracy: {GREEN}{leniant_acc}{OFF}, Strict Accuracy: {GREEN}{strict_acc}{OFF}, Mean Reciprocal Rank (MRR): {GREEN}{average_mrr}{OFF}\")\n",
    "    print ('\\n')\n",
    "    return leniant_acc,strict_acc,average_mrr\n",
    "\n",
    "# do_factoid_eval(gold_df,'tmp/qa/factoid/BioASQform_BioASQ-answer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_list_eval(gold_df,gen_df):\n",
    "    print(f\"{CYAN}List Evaluation{OFF}\")\n",
    "    list_gold_df = gold_df[gold_df['type'] == 'list']\n",
    "    list_gen_df = gen_df[gen_df['type'] == 'list']\n",
    "\n",
    "    if DEBUG:\n",
    "        print(f\" [{len(list_gold_df)}] Gold List Questions\")\n",
    "        print(f\" [{len(list_gen_df)}] Generated List Questions\")\n",
    "    \n",
    "    gold_ids,gold_ans,gen_ids,gen_ans = get_col_list(list_gold_df,gen_df,'exact_answer')\n",
    "    num_gen_q_without_ans = 0\n",
    "    num_gold_q_without_ans = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    num_gen = 0\n",
    "    scores = []\n",
    "    # for each question\n",
    "    for i in range (len(gold_ids)):\n",
    "        gold_val = gold_ans[i]\n",
    "        if gold_val == []:\n",
    "            num_gold_q_without_ans += 1\n",
    "            continue\n",
    "        gen_val = gen_ans[gen_ids.index(gold_ids[i])]\n",
    "        # if answers are found\n",
    "        if gen_val != None: # List is only able to find a single item list\n",
    "            # TP is answer in Gold AND Gen\n",
    "            # FP is answer NOT IN GOLD, but YES IN GEN\n",
    "            # FN is answer IN Gold but NOT GEN\n",
    "            gold_list = gold_val[0]\n",
    "            for val in gold_list:\n",
    "                if val in gen_val:\n",
    "                    tp += 1\n",
    "                elif val not in gen_val:\n",
    "                    fn += 1\n",
    "            for val in gen_val:\n",
    "                if val not in gold_list:\n",
    "                    fp += 1\n",
    "            num_gen += 1\n",
    "            f1,p,r = get_f1_p_r(tp,fp,fn, tag =\"List Questions\")\n",
    "            scores.append((f1,p,r))\n",
    "    #sanity check\n",
    "    print(f\"{GREEN}[{len(gold_ids) - num_gold_q_without_ans}/{len(gold_ids)}]{OFF} List questions have answers in gold dataset\")\n",
    "    print(f\"{GREEN}[{num_gen}/{len(list_gen_df)}]{OFF} List questions have answers in generated dataset\")\n",
    "\n",
    "    # OVERALL SCORES\n",
    "    f1_sum = p_sum = r_sum = 0\n",
    "    for f1,p,r in scores:\n",
    "        f1_sum += f1\n",
    "        p_sum += p\n",
    "        r_sum += r\n",
    "    f1_sum /= len(scores)\n",
    "    p_sum /= len(scores)\n",
    "    r_sum /= len(scores)\n",
    "\n",
    "    print (f'List Questions mean f1 {GREEN}{f1_sum}{OFF}, precision {GREEN}{p_sum}{OFF}, recall {GREEN}{r_sum}{OFF}')\n",
    "    print ('\\n')\n",
    "    return f1_sum,p_sum,r_sum,scores\n",
    "\n",
    "# list_report = do_list_eval(gold_df,gen_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_gold_qu_output(gold_df,qu_generated):\n",
    "    new_file_name = qu_generated.replace(\".xml\",\"_GOLD.xml\")\n",
    "\n",
    "    fileTree = et.parse(qu_generated)\n",
    "    if fileTree:\n",
    "        root = fileTree.getroot()\n",
    "        questions = root.findall('Q')\n",
    "        for question in questions:\n",
    "            id = question.attrib.get(\"id\")\n",
    "            qp = question.find(\"QP\")\n",
    "            # remove type and entities\n",
    "            qp.clear()\n",
    "            # type is the fifth element in the row\n",
    "            gold_type = gold_df.loc[gold_df['id'] == id].values[0][4]\n",
    "            type_ele = et.SubElement(qp,'Type')\n",
    "            type_ele.text = gold_type\n",
    "            gold_concepts = gold_df.loc[gold_df['id'] == id].values[0][7]\n",
    "            ent_list = []\n",
    "            if not isinstance(gold_concepts,list):\n",
    "                if DEBUG:\n",
    "                    print(f\"Question [{id}] has no golden human_concepts [{type(gold_concepts)}]\")\n",
    "                continue\n",
    "            for ent in gold_concepts:\n",
    "                ent_list.append(str(ent))\n",
    "                qp_en = et.SubElement(qp,'Entities') \n",
    "                qp_en.text = str(ent)\n",
    "            qp_query = et.SubElement(qp,'Query')\n",
    "            qp_query.text = str(' '.join(ent_list))\n",
    "        tree = et.ElementTree(root)\n",
    "        os.makedirs(os.path.dirname(new_file_name), exist_ok=True)\n",
    "        print(f\"Writing gold QU output / IR input to {new_file_name}\")\n",
    "        tree.write(new_file_name, pretty_print=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_gols_ir_output(gold_df,ir_generated):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_intermediary_datasets(gold_df,qu_output,ir_output):\n",
    "    # QU: replace concepts with human_concepts and type with gold type\n",
    "    gen_gold_qu_output(gold_df,qu_output)\n",
    "    # IR: Replace IR with full_abstract in document format\n",
    "    gen_gols_ir_output(gold_df,ir_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mConcepts Evaluation\u001b[0m\n",
      "\u001b[32m[1607/3243]\u001b[0m Questions have human readable concepts in gold dataset\n",
      "\u001b[32m[3242/3243]\u001b[0m Questions have human readable concepts in generated dataset\n",
      "Concepts mean f1 \u001b[32m0.507613033876486\u001b[0m, precision \u001b[32m0.6096832682725072\u001b[0m, recall \u001b[32m0.43495543509698553\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[36mPubMed Documents Evaluation\u001b[0m\n",
      "\u001b[32m[3243/3243]\u001b[0m Questions have documents in gold dataset\n",
      "\u001b[32m[1947/3243]\u001b[0m Questions have documents in generated dataset\n",
      "PubMed Documents mean f1 \u001b[32m0.5822856764616713\u001b[0m, precision \u001b[32m0.5808496837782734\u001b[0m, recall \u001b[32m0.5837572783823382\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[36mYes/No Evaluation\u001b[0m\n",
      "Yes | True Posative: \u001b[32m0\u001b[0m, False Posative: \u001b[32m0\u001b[0m, False Negative: \u001b[32m427\u001b[0m\n",
      "Yes | f1 \u001b[32m0\u001b[0m, precision \u001b[32m0\u001b[0m, recall \u001b[32m0.0\u001b[0m\n",
      "No | True Posative: \u001b[32m78\u001b[0m, False Posative: \u001b[32m427\u001b[0m, False Negative: \u001b[32m0\u001b[0m\n",
      "No | f1 \u001b[32m0.26758147512864494\u001b[0m, precision \u001b[32m0.15445544554455445\u001b[0m, recall \u001b[32m1.0\u001b[0m\n",
      "Overall Yes/No | f1 \u001b[32m0.13379073756432247\u001b[0m, precision \u001b[32m0.07722772277227723\u001b[0m, recall \u001b[32m0.5\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[36mFactoid Evaluation\u001b[0m\n",
      "\u001b[32m[941/941]\u001b[0m Factoid questions have answers in gold dataset\n",
      "\u001b[32m[460/493]\u001b[0m Factoid questions have answers in generated dataset\n",
      "Leniant Accuracy: \u001b[32m0.0\u001b[0m, Strict Accuracy: \u001b[32m0.0\u001b[0m, Mean Reciprocal Rank (MRR): \u001b[32m0.0\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[36mList Evaluation\u001b[0m\n",
      "\u001b[32m[644/644]\u001b[0m List questions have answers in gold dataset\n",
      "\u001b[32m[395/647]\u001b[0m List questions have answers in generated dataset\n",
      "List Questions mean f1 \u001b[32m0.0007866310930120968\u001b[0m, precision \u001b[32m0.00040072530991740955\u001b[0m, recall \u001b[32m0.021403064121746397\u001b[0m\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3243 entries, 0 to 3242\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   body            3243 non-null   object\n",
      " 1   documents       3243 non-null   object\n",
      " 2   ideal_answer    3243 non-null   object\n",
      " 3   concepts        1854 non-null   object\n",
      " 4   type            3243 non-null   object\n",
      " 5   id              3243 non-null   object\n",
      " 6   snippets        3243 non-null   object\n",
      " 7   human_concepts  1854 non-null   object\n",
      " 8   full_abstracts  3243 non-null   object\n",
      " 9   titles          3243 non-null   object\n",
      " 10  triples         322 non-null    object\n",
      " 11  exact_answer    2466 non-null   object\n",
      "dtypes: object(12)\n",
      "memory usage: 304.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Set up the golden answer dataframe\n",
    "golden_dataset_path = \"testing_datasets/augmented_concepts_abstracts_titles.json\"\n",
    "generated_qu = \"tmp/ir/output/bioasq_qa.xml\"\n",
    "with open(golden_dataset_path,'r') as f:\n",
    "    gold_data = json.loads(f.read())\n",
    "# load and flatten data\n",
    "gold_df = pd.json_normalize(gold_data,record_path=\"questions\")\n",
    "# get gold df\n",
    "gold_df['documents'] = gold_df['documents'].apply(get_pmid)\n",
    "# get generated df\n",
    "\n",
    "gen_df = parse_xml(generated_qu,'tmp/qa')\n",
    "\n",
    "## Fully Generated\n",
    "# Concepts\n",
    "concepts_report = do_concepts_eval(gold_df,gen_df)\n",
    "#print(concepts_report)\n",
    "\n",
    "# Documents \n",
    "pmids_report = do_pmids_eval(gold_df,gen_df)\n",
    "# print(pmids_report)\n",
    "\n",
    "# Type\n",
    "type_report = classification_report(gold_df['type'].to_numpy(),gen_df['type'].to_numpy(),output_dict=DEBUG)\n",
    "# print(type_report)\n",
    "\n",
    "# Yes/No Question Answering\n",
    "yes_no_report = do_yes_no_eval(gold_df,gen_df)\n",
    "#print(yes_no_report)\n",
    "\n",
    "# Factoid Question Answering\n",
    "factoid_report = do_factoid_eval(gold_df,'tmp/qa/factoid/BioASQform_BioASQ-answer.json')\n",
    "#print(factoid_report)\n",
    "\n",
    "# List Question Answering\n",
    "list_report = do_list_eval(gold_df,gen_df)\n",
    "# print(list_report)\n",
    "## Mixed Gold\n",
    "\n",
    "## All gold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Genes, erbB-1', 'Ligands']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gold_df['human_concepts']\n",
    "gold_df.loc[gold_df['id'] == '55046d5ff8aee20f27000007'].values[0][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question [54e25eaaae9738404b000017] has no golden human_concepts [<class 'float'>]\n",
      "Question [5709e4b2cf1c32585100001c] has no golden human_concepts [<class 'float'>]\n",
      "Question [56bc751eac7ad10019000013] has no golden human_concepts [<class 'float'>]\n",
      "Question [54d4e03a3706e89528000001] has no golden human_concepts [<class 'float'>]\n",
      "Writing gold QU output / IR input to tmp/small_batch/ir/input/bioasq_qa_SMALL_GOLD.xml\n"
     ]
    }
   ],
   "source": [
    "gen_gold_qu_output(gold_df,\"tmp/small_batch/ir/input/bioasq_qa_SMALL.xml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "80370dbcfc3023cdbcb71d202b3d45870b2ec245031d0411f339cfb8d50c0055"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('3.7.9': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
