{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree as et\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "\n",
    "from utils import *\n",
    "\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pmid(docs):\n",
    "    documents = [\n",
    "        document.split(\"/\")[-1] for document in docs\n",
    "    ]\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers(answers_files):\n",
    "    all_answers = dict()\n",
    "    for a in answers_files:\n",
    "        with open(a, \"r\") as f:\n",
    "            d = json.loads(f.read())\n",
    "            if DEBUG:\n",
    "                print(f\"{len(d)} answers found in {a}\")\n",
    "            for key, value in d.items():\n",
    "                if key in all_answers.keys():\n",
    "                    print(f\"MULTIPLE ANSWERS FOR {key}\")\n",
    "                if isinstance(value,list):\n",
    "                    all_answers[key] = value[0] # get the first value which is answer not prediction for the yes/no\n",
    "                else:\n",
    "                    all_answers[key] = value\n",
    "    return all_answers\n",
    "\n",
    "def get_three_files(a_dir):\n",
    "    return [\n",
    "        a_dir + \"/factoid/predictions.json\",\n",
    "        a_dir + \"/list/predictions.json\",\n",
    "        a_dir + \"/yesno/predictions.json\",\n",
    "    ]\n",
    "\n",
    "def get_col_list(gold_df,gen_df,col):\n",
    "    gold_col = gold_df.loc[:,['id',col]].copy()\n",
    "    gen_col = gen_df.loc[:,['id',col]].copy() \n",
    "    \n",
    "    gold = gold_col.to_dict(orient='list')\n",
    "    gen = gen_col.to_dict(orient='list')\n",
    "    gen_ids = gen['id']\n",
    "    gen_vals = gen[col]\n",
    "    gold_ids = gold['id']\n",
    "    gold_vals = gold[col]\n",
    "    return gold_ids,gold_vals,gen_ids,gen_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_xml(xml_file, dir_for_qa): \n",
    "    no_answers = 0\n",
    "    # get answers\n",
    "    qa_answers = get_answers(get_three_files(dir_for_qa))\n",
    "    # get ir and qu\n",
    "    df_cols = ['id','human_concepts','documents','full_abstracts','titles','type', 'exact_answer']\n",
    "    xtree = et.parse(xml_file)\n",
    "    xroot = xtree.getroot()\n",
    "    rows = []\n",
    "    for question in xroot: \n",
    "        id = question.attrib.get(\"id\")\n",
    "        ir = question.find(\"IR\")\n",
    "        qp = question.find(\"QP\")\n",
    "        concepts = [e.text for e in qp.findall(\"Entities\")]\n",
    "        qa_type = qp.find(\"Type\").text\n",
    "        titles =  [e.find(\"Title\").text for e in ir.findall(\"Result\")]\n",
    "        abstracts =  [e.find(\"Abstract\").text for e in ir.findall(\"Result\")]\n",
    "        pmids = [e.get(\"PMID\") for e in ir.findall(\"Result\")]\n",
    "        exact_answer = qa_answers[id] if id in qa_answers else None\n",
    "        if DEBUG and not exact_answer:\n",
    "            print(f\"id [{id}] has no answer\")\n",
    "            no_answers +=1\n",
    "        rows.append({\"id\":id,\"human_concepts\":concepts,\"documents\":pmids,\"full_abstracts\":abstracts,\"titles\":titles,\"type\":qa_type,'exact_answer':exact_answer})\n",
    "    out_df = pd.DataFrame(rows, columns=df_cols)\n",
    "    if DEBUG:\n",
    "        print(f\"[{no_answers}/{len(out_df)}] questions had answers\")\n",
    "    return out_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_yes_no_info(df, tag):\n",
    "    print(tag)\n",
    "    print(f\" [{len(df)}] {tag} Yes/No Questions\")\n",
    "    yes_df = df[df['exact_answer'] == 'yes']\n",
    "    no_df = df[df['exact_answer'] == 'no']\n",
    "    print(f\" [{len(yes_df)}] {tag} Yes Questions\")\n",
    "    print(f\" [{len(no_df)}] {tag} No Questions\")\n",
    "\n",
    "\n",
    "\"\"\" f1 Yes\n",
    "    tp is gen 'yes' | gold 'yes'\n",
    "    fp is gen 'yes' | gold 'no'\n",
    "    fn is gen 'no' |  gold 'yes'\n",
    "\n",
    "    f1 No\n",
    "    tp is gen 'no' | gold 'no'\n",
    "    fp is gen 'no' | gold 'yes'\n",
    "    fn is gen 'yes' |  gold 'no'\n",
    "\n",
    "    IGNORE if the predicted type is yes/no but gold type is different\n",
    "\"\"\"\n",
    "def do_yes_no_eval(gold_df,gen_df):\n",
    "    print(\"Yes/No Evaluation\")\n",
    "    yes_no_gold_df = gold_df[gold_df['type'] == 'yesno']\n",
    "    yes_no_gen_df = gen_df[gen_df['type'] == 'yesno']\n",
    "    \n",
    "    if DEBUG:\n",
    "        # Gold stats\n",
    "        print_yes_no_info(yes_no_gold_df, \"Gold\")\n",
    "        # Gen Stats\n",
    "        print_yes_no_info(yes_no_gen_df, \"Generated\")\n",
    "\n",
    "    gold_ids,gold_ans,gen_ids,gen_ans = get_col_list(yes_no_gold_df,gen_df,'exact_answer')\n",
    "\n",
    "    # YES \n",
    "    ytp = 0\n",
    "    yfp = 0\n",
    "    yfn = 0\n",
    "\n",
    "    for i in range (len(gold_ids)):\n",
    "        gold_val = gold_ans[i]\n",
    "        gen_val = gen_ans[gen_ids.index(gold_ids[i])]\n",
    "        if gen_val:\n",
    "            if gold_val == 'yes':\n",
    "                if gen_val =='yes':\n",
    "                    ytp += 1\n",
    "                elif gen_val =='no':\n",
    "                    yfn += 1\n",
    "                else:\n",
    "                    if DEBUG:\n",
    "                        print(f\"yes question [{gold_ids[i]}] had generated answer {gen_val}\")\n",
    "            elif gold_val == 'no':\n",
    "                if gen_val == 'yes':\n",
    "                    yfp +=1\n",
    "                elif gen_val =='no':\n",
    "                    pass #handled by no f1\n",
    "                else:\n",
    "                    if DEBUG:\n",
    "                        print(f\"no question [{gold_ids[i]}] had generated answer {gen_val}\")\n",
    "            else:\n",
    "                print(f\"GOLDEN answer to yes/no question [{gold_ids[i]}] was {gold_val}\")\n",
    "                \n",
    "        else: # not identified as yes/no question by generated\n",
    "            pass\n",
    "    #sanity check\n",
    "    print (f\"Yes | True Posative: {ytp}, False Posative: {yfp}, False Negative: {yfn}\")\n",
    "    try:\n",
    "        yp = ytp/(ytp + yfp)\n",
    "    except:\n",
    "        yp = 0\n",
    "    try:\n",
    "        yr = ytp/(ytp + yfn)\n",
    "    except:\n",
    "        yr = 0\n",
    "    try:\n",
    "        yf1 = 2 * ((yp * yr)/(yp+yr))\n",
    "    except:\n",
    "        yf1 = 0\n",
    "    print (f'Yes | f1 {yf1}, precision {yp}, recall {yr}')\n",
    "\n",
    "    # NO SIDE\n",
    "    ntp = 0\n",
    "    nfp = 0\n",
    "    nfn = 0\n",
    "\n",
    "    for i in range (len(gold_ids)):\n",
    "        gold_val = gold_ans[i]\n",
    "        gen_val = gen_ans[gen_ids.index(gold_ids[i])]\n",
    "        if gen_val:\n",
    "            if gold_val == 'no':\n",
    "                if gen_val =='no':\n",
    "                    ntp += 1\n",
    "                elif gen_val =='yes':\n",
    "                    nfn += 1\n",
    "                else:\n",
    "                    if DEBUG:\n",
    "                        print(f\"no question [{gold_ids[i]}] had generated answer {gen_val}\")\n",
    "            elif gold_val == 'yes':\n",
    "                if gen_val == 'no':\n",
    "                    nfp +=1\n",
    "                elif gen_val =='yes':\n",
    "                    pass #handled by no f1\n",
    "                else:\n",
    "                    print(f\"yes question [{gold_ids[i]}] had generated answer {gen_val}\")\n",
    "            else:\n",
    "                if DEBUG:\n",
    "                    print(f\"GOLDEN answer to yes/no question [{gold_ids[i]}] was {gold_val}\")\n",
    "                \n",
    "        else: # not identified as yes/no question by generated\n",
    "            pass\n",
    "    \n",
    "    # sanity check\n",
    "    print (f\"No | True Posative: {ntp}, False Posative: {nfp}, False Negative: {nfn}\")\n",
    "    try:\n",
    "        np = ntp/(ntp + nfp)\n",
    "    except:\n",
    "        np = 0\n",
    "    try:\n",
    "        nr = ntp/(ntp + nfn)\n",
    "    except:\n",
    "        nr = 0\n",
    "    try:\n",
    "        nf1 = 2 * ((np * nr)/(np+nr))\n",
    "    except:\n",
    "        nf1 = 0\n",
    "    print (f'No | f1 {nf1}, precision {np}, recall {nr}')\n",
    "\n",
    "    f1 = (yf1 + nf1)/2 \n",
    "    p = (yp + np)/2 \n",
    "    r = (yr + nr)/2 \n",
    "    print(f\"Overall Yes/No | f1 {f1}, precision {p}, recall {r}\")\n",
    "    print ('\\n')\n",
    "    return yf1,yp,yr,nf1,np,nr,f1\n",
    "\n",
    "# yes_no_report = do_yes_no_eval(gold_df,gen_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute [[Mean average precision, Geometric mean average precision]], precision, recall, f1 score\n",
    "def do_concepts_eval(gold_df,gen_df):\n",
    "    print(\"Concepts Evaluation\")\n",
    "    gold_ids,gold_cons,gen_ids,gen_cons = get_col_list(gold_df,gen_df,'human_concepts')\n",
    "    num_gen_q_without_cons = 0\n",
    "    num_gold_q_without_cons = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    scores = []\n",
    "    # for each question\n",
    "    for i in range (len(gold_ids)):\n",
    "        gold_val = gold_cons[i]\n",
    "        if not isinstance(gold_val,list) or gold_val == []:\n",
    "            num_gold_q_without_cons += 1\n",
    "            continue\n",
    "        gen_val = gen_cons[gen_ids.index(gold_ids[i])]\n",
    "        # if concepts are found\n",
    "        if gen_val != []:\n",
    "            # TP is concept in Gold AND Gen\n",
    "            # FP is concept NOT IN GOLD, but YES IN GEN\n",
    "            # FN is concept IN Gold but NOT GEN\n",
    "\n",
    "            # get unique concepts from both gold and gen\n",
    "            unique_gold_cons = set(gold_val[0])\n",
    "            unique_gen_cons = set(gen_val[0])\n",
    "            for val in unique_gold_cons:\n",
    "                if val in unique_gen_cons:\n",
    "                    tp += 1\n",
    "                elif val not in unique_gen_cons:\n",
    "                    fn += 1\n",
    "            for val in unique_gen_cons:\n",
    "                if val not in unique_gold_cons:\n",
    "                    fp += 1\n",
    "\n",
    "            f1,p,r = get_f1_p_r(tp,fp,fn, tag =\"Concepts\")\n",
    "            scores.append((f1,p,r))\n",
    "        else: # There are no concepts retrieved for this document\n",
    "            num_gen_q_without_cons += 1\n",
    "            pass\n",
    "    #sanity check\n",
    "    print(f\"[{len(gold_ids) - num_gold_q_without_cons}/{len(gold_ids)}] Questions have human readable concepts in gold dataset\")\n",
    "    print(f\"[{len(gen_ids) - num_gen_q_without_cons}/{len(gen_ids)}] Questions have human readable concepts in generated dataset\")\n",
    "\n",
    "    # OVERALL SCORES\n",
    "    f1_sum = p_sum = r_sum = 0\n",
    "    for f1,p,r in scores:\n",
    "        f1_sum += f1\n",
    "        p_sum += p\n",
    "        r_sum += r\n",
    "    f1_sum /= len(scores)\n",
    "    p_sum /= len(scores)\n",
    "    r_sum /= len(scores)\n",
    "\n",
    "    print (f'Concepts mean f1 {f1_sum}, precision {p_sum}, recall {r_sum}')\n",
    "    print ('\\n')\n",
    "    return f1_sum,p_sum,r_sum,scores\n",
    "\n",
    "# concepts_report = do_concepts_eval(gold_df,gen_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1_p_r(tp,fp,fn, tag =\"calculated\"):\n",
    "    if DEBUG:\n",
    "        print (f\"{tag} tp: {tp}, fp: {fp}, fn: {fn}\")\n",
    "    try:\n",
    "        p = tp/(tp + fp)\n",
    "    except:\n",
    "        p = 0\n",
    "    try:\n",
    "        r = tp/(tp + fn)\n",
    "    except:\n",
    "        r = 0\n",
    "    try:\n",
    "        f1 = 2 * ((p * r)/(p+r))\n",
    "    except:\n",
    "        f1 = 0\n",
    "    if DEBUG:\n",
    "        print (f'{tag} f1 {f1}, precision {p}, recall {r}')\n",
    "    return f1,p,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_pmids_eval(gold_df,gen_df):\n",
    "    print(\"PubMed Documents Evaluation\")\n",
    "    # pmids are the pubmed document ids\n",
    "    gold_ids,gold_pmids,gen_ids,gen_pmids = get_col_list(gold_df,gen_df,'documents')\n",
    "    num_gen_q_without_docs = 0\n",
    "    num_gold_q_without_docs = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    scores = []\n",
    "    # for each question\n",
    "    for i in range (len(gold_ids)):\n",
    "        gold_val = gold_pmids[i]\n",
    "        if gold_val == []:\n",
    "            num_gold_q_without_docs += 1\n",
    "            continue\n",
    "        gen_val = gen_pmids[gen_ids.index(gold_ids[i])]\n",
    "        # if documents are found\n",
    "        if gen_val != []:\n",
    "            # TP is pmid in Gold AND Gen\n",
    "            # FP is pmid NOT IN GOLD, but YES IN GEN\n",
    "            # FN is pmid IN Gold but NOT GEN\n",
    "\n",
    "            # get unique PMIDs from both gold and gen\n",
    "            unique_gold_pmids = set(gold_val[0])\n",
    "            unique_gen_pmids = set(gen_val[0])\n",
    "            for val in unique_gold_pmids:\n",
    "                if val in unique_gen_pmids:\n",
    "                    tp += 1\n",
    "                elif val not in unique_gen_pmids:\n",
    "                    fn += 1\n",
    "            for val in unique_gen_pmids:\n",
    "                if val not in unique_gold_pmids:\n",
    "                    fp += 1\n",
    "\n",
    "            f1,p,r = get_f1_p_r(tp,fp,fn, tag =\"PubMed Documents\")\n",
    "            scores.append((f1,p,r))\n",
    "        else: # There are no documents retrieved for this document\n",
    "            num_gen_q_without_docs += 1\n",
    "            pass\n",
    "    #sanity check\n",
    "    print(f\"[{len(gold_ids) - num_gold_q_without_docs}/{len(gold_ids)}] Questions have documents in gold dataset\")\n",
    "    print(f\"[{len(gen_ids) - num_gen_q_without_docs}/{len(gen_ids)}] Questions have documents in generated dataset\")\n",
    "\n",
    "    # OVERALL SCORES\n",
    "    f1_sum = p_sum = r_sum = 0\n",
    "    for f1,p,r in scores:\n",
    "        f1_sum += f1\n",
    "        p_sum += p\n",
    "        r_sum += r\n",
    "    f1_sum /= len(scores)\n",
    "    p_sum /= len(scores)\n",
    "    r_sum /= len(scores)\n",
    "\n",
    "    print (f'PubMed Documents mean f1 {f1_sum}, precision {p_sum}, recall {r_sum}')\n",
    "    print ('\\n')\n",
    "    return f1_sum,p_sum,r_sum,scores\n",
    "\n",
    "# pmid_report = do_pmids_eval(gold_df,gen_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use strict and leniant accuracy  (first result, or any result)\n",
    "def do_factoid_eval(gold_df,gen_factoid_path):\n",
    "    print(\"Factoid Evaluation\")\n",
    "    factoid_gold_df = gold_df[gold_df['type'] == 'factoid']\n",
    "    factoid_gen_df = gen_df[gen_df['type'] == 'factoid']\n",
    "\n",
    "    if DEBUG:\n",
    "        print(f\" [{len(factoid_gold_df)}] Gold Factoid Questions\")\n",
    "        print(f\" [{len(factoid_gen_df)}] Generated Factoid Questions\")\n",
    "    gold_ids,gold_ans,gen_ids,gen_ans = get_col_list(factoid_gold_df,gen_df,'exact_answer')\n",
    "\n",
    "    # Use alternative strategy to handle ranked factoid preds\n",
    "    with open(gen_factoid_path, \"r\") as ft_file:\n",
    "        factoid_gen_json = json.load(ft_file)\n",
    "    \n",
    "    gen_factoid_answers = {}\n",
    "    for question in factoid_gen_json[\"questions\"]:\n",
    "        id = question[\"id\"]\n",
    "        if len(id) == 24:\n",
    "            id = id[0:20]\n",
    "        answer = question['exact_answer']\n",
    "        if isinstance(answer,list): \n",
    "            if isinstance(answer[0],list): # handle list in list\n",
    "                answer = [e[0] for e in answer]\n",
    "        gen_factoid_answers[id] = answer\n",
    "\n",
    "    num_gold_q_without_ans = 0\n",
    "    num_strict = 0\n",
    "    num_leniant = 0\n",
    "    num_total = 0\n",
    "    mrrs = []\n",
    "    # for each question\n",
    "    for i in range (len(gold_ids)):\n",
    "        gold_val = gold_ans[i][0]\n",
    "        if gold_val == []:\n",
    "            num_gold_q_without_ans += 1\n",
    "            continue\n",
    "        # trim last 4 digits which get removed for the final bioasq form answers\n",
    "        trimmed_id = gold_ids[i][0:20]\n",
    "        if trimmed_id not in gen_factoid_answers.keys():\n",
    "            if DEBUG:\n",
    "                print(f\"{trimmed_id} wasn't correctly identified as factoid\")\n",
    "            continue\n",
    "        gen_vals = gen_factoid_answers[trimmed_id]\n",
    "        gen_vals_clean = [e.lower().strip() for e in gen_vals]\n",
    "        if DEBUG:\n",
    "            print(gold_val,\" | \",gen_vals)\n",
    "        # accuracy calculations\n",
    "        gold_val_clean = gold_val\n",
    "        num_total += 1\n",
    "        if gold_val_clean == gen_vals_clean[0]: # force lowercase / strip whitespace to help\n",
    "            num_strict += 1\n",
    "            num_leniant += 1\n",
    "        elif gold_val_clean in gen_vals_clean:\n",
    "            num_leniant += 1\n",
    "\n",
    "        # mrr calculations\n",
    "        mrr = 0\n",
    "        r = 0\n",
    "        n = len(gen_vals_clean)\n",
    "        for i in range(1,n+1):\n",
    "            if gen_vals_clean[i-1] == gold_val_clean:\n",
    "                r = i\n",
    "                break\n",
    "        if r != 0:\n",
    "            mrr = 1/n * 1/r \n",
    "            if DEBUG:\n",
    "                print(f\"{trimmed_id} MRR: {mrr}\")\n",
    "        mrrs.append(mrr)\n",
    "\n",
    "    average_mrr = sum(mrrs) / len(mrrs)\n",
    "    leniant_acc = num_leniant/num_total\n",
    "    strict_acc = num_strict/num_total\n",
    "\n",
    "    # sanity check\n",
    "    print(f\"[{len(gold_ids) - num_gold_q_without_ans}/{len(gold_ids)}] Factoid questions have answers in gold dataset\")\n",
    "    print(f\"[{num_total}/{len(gen_factoid_answers)}] Factoid questions have answers in generated dataset\")\n",
    "    print(f\"Leniant Accuracy: {leniant_acc}, Strict Accuracy: {strict_acc}, Mean Reciprocal Rank (MRR): {average_mrr}\")\n",
    "    print ('\\n')\n",
    "    return leniant_acc,strict_acc,average_mrr\n",
    "\n",
    "# do_factoid_eval(gold_df,'tmp/qa/factoid/BioASQform_BioASQ-answer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List Evaluation\n",
      "[644/644] List questions have answers in gold dataset\n",
      "[395/647] List questions have answers in generated dataset\n",
      "List Questions mean f1 0.0007866310930120968, precision 0.00040072530991740955, recall 0.021403064121746397\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def do_list_eval(gold_df,gen_df):\n",
    "    print(\"List Evaluation\")\n",
    "    list_gold_df = gold_df[gold_df['type'] == 'list']\n",
    "    list_gen_df = gen_df[gen_df['type'] == 'list']\n",
    "\n",
    "    if DEBUG:\n",
    "        print(f\" [{len(list_gold_df)}] Gold List Questions\")\n",
    "        print(f\" [{len(list_gen_df)}] Generated List Questions\")\n",
    "    \n",
    "    gold_ids,gold_ans,gen_ids,gen_ans = get_col_list(list_gold_df,gen_df,'exact_answer')\n",
    "    num_gen_q_without_ans = 0\n",
    "    num_gold_q_without_ans = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    num_gen = 0\n",
    "    scores = []\n",
    "    # for each question\n",
    "    for i in range (len(gold_ids)):\n",
    "        gold_val = gold_ans[i]\n",
    "        if gold_val == []:\n",
    "            num_gold_q_without_ans += 1\n",
    "            continue\n",
    "        gen_val = gen_ans[gen_ids.index(gold_ids[i])]\n",
    "        # if answers are found\n",
    "        if gen_val != None: # List is only able to find a single item list\n",
    "            # TP is answer in Gold AND Gen\n",
    "            # FP is answer NOT IN GOLD, but YES IN GEN\n",
    "            # FN is answer IN Gold but NOT GEN\n",
    "            gold_list = gold_val[0]\n",
    "            for val in gold_list:\n",
    "                if val in gen_val:\n",
    "                    tp += 1\n",
    "                elif val not in gen_val:\n",
    "                    fn += 1\n",
    "            for val in gen_val:\n",
    "                if val not in gold_list:\n",
    "                    fp += 1\n",
    "            num_gen += 1\n",
    "            f1,p,r = get_f1_p_r(tp,fp,fn, tag =\"List Questions\")\n",
    "            scores.append((f1,p,r))\n",
    "    #sanity check\n",
    "    print(f\"[{len(gold_ids) - num_gold_q_without_ans}/{len(gold_ids)}] List questions have answers in gold dataset\")\n",
    "    print(f\"[{num_gen}/{len(list_gen_df)}] List questions have answers in generated dataset\")\n",
    "\n",
    "    # OVERALL SCORES\n",
    "    f1_sum = p_sum = r_sum = 0\n",
    "    for f1,p,r in scores:\n",
    "        f1_sum += f1\n",
    "        p_sum += p\n",
    "        r_sum += r\n",
    "    f1_sum /= len(scores)\n",
    "    p_sum /= len(scores)\n",
    "    r_sum /= len(scores)\n",
    "\n",
    "    print (f'List Questions mean f1 {f1_sum}, precision {p_sum}, recall {r_sum}')\n",
    "    print ('\\n')\n",
    "    return f1_sum,p_sum,r_sum,scores\n",
    "\n",
    "list_report = do_list_eval(gold_df,gen_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts Evaluation\n",
      "[1607/3243] Questions have human readable concepts in gold dataset\n",
      "[3242/3243] Questions have human readable concepts in generated dataset\n",
      "Concepts mean f1 0.507613033876486, precision 0.6096832682725072, recall 0.43495543509698553\n",
      "\n",
      "\n",
      "PubMed Documents Evaluation\n",
      "[3243/3243] Questions have documents in gold dataset\n",
      "[1947/3243] Questions have documents in generated dataset\n",
      "PubMed Documents mean f1 0.5822856764616713, precision 0.5808496837782734, recall 0.5837572783823382\n",
      "\n",
      "\n",
      "Yes/No Evaluation\n",
      "Yes | True Posative: 0, False Posative: 0, False Negative: 427\n",
      "Yes | f1 0, precision 0, recall 0.0\n",
      "No | True Posative: 78, False Posative: 427, False Negative: 0\n",
      "No | f1 0.26758147512864494, precision 0.15445544554455445, recall 1.0\n",
      "Overall Yes/No | f1 0.13379073756432247, precision 0.07722772277227723, recall 0.5\n",
      "\n",
      "\n",
      "Factoid Evaluation\n",
      "[941/941] Factoid questions have answers in gold dataset\n",
      "[460/493] Factoid questions have answers in generated dataset\n",
      "Leniant Accuracy: 0.0, Strict Accuracy: 0.0, Mean Reciprocal Rank (MRR): 0.0\n",
      "\n",
      "\n",
      "List Evaluation\n",
      "[644/644] List questions have answers in gold dataset\n",
      "[395/647] List questions have answers in generated dataset\n",
      "List Questions mean f1 0.0007866310930120968, precision 0.00040072530991740955, recall 0.021403064121746397\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up the golden answer dataframe\n",
    "golden_dataset_path = \"testing_datasets/augmented_concepts_abstracts_titles.json\"\n",
    "generated_qu = \"tmp/ir/output/bioasq_qa.xml\"\n",
    "with open(golden_dataset_path,'r') as f:\n",
    "    gold_data = json.loads(f.read())\n",
    "# load and flatten data\n",
    "gold_df = pd.json_normalize(gold_data,record_path=\"questions\")\n",
    "# get gold df\n",
    "gold_df['documents'] = gold_df['documents'].apply(get_pmid)\n",
    "# get generated df\n",
    "\n",
    "gen_df = parse_xml(generated_qu,'tmp/qa')\n",
    "\n",
    "## Fully Generated\n",
    "# Concepts\n",
    "concepts_report = do_concepts_eval(gold_df,gen_df)\n",
    "#print(concepts_report)\n",
    "\n",
    "# Documents \n",
    "pmids_report = do_pmids_eval(gold_df,gen_df)\n",
    "# print(pmids_report)\n",
    "\n",
    "# Type\n",
    "type_report = classification_report(gold_df['type'].to_numpy(),gen_df['type'].to_numpy(),output_dict=DEBUG)\n",
    "# print(type_report)\n",
    "\n",
    "# Yes/No Question Answering\n",
    "yes_no_report = do_yes_no_eval(gold_df,gen_df)\n",
    "#print(yes_no_report)\n",
    "\n",
    "# Factoid Question Answering\n",
    "factoid_report = do_factoid_eval(gold_df,'tmp/qa/factoid/BioASQform_BioASQ-answer.json')\n",
    "#print(factoid_report)\n",
    "\n",
    "# List Question Answering\n",
    "list_report = do_list_eval(gold_df,gen_df)\n",
    "# print(list_report)\n",
    "## Mixed Gold\n",
    "\n",
    "## All gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "80370dbcfc3023cdbcb71d202b3d45870b2ec245031d0411f339cfb8d50c0055"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('3.7.9': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
