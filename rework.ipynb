{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree as et\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "\n",
    "from utils import *\n",
    "\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pmid(docs):\n",
    "    documents = [\n",
    "        document.split(\"/\")[-1] for document in docs\n",
    "    ]\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers(answers_files):\n",
    "    all_answers = dict()\n",
    "    for a in answers_files:\n",
    "        with open(a, \"r\") as f:\n",
    "            d = json.loads(f.read())\n",
    "            if DEBUG:\n",
    "                print(f\"{len(d)} answers found in {a}\")\n",
    "            for key, value in d.items():\n",
    "                if key in all_answers.keys():\n",
    "                    print(f\"MULTIPLE ANSWERS FOR {key}\")\n",
    "                if isinstance(value,list):\n",
    "                    all_answers[key] = value[0] # get the first value which is answer not prediction for the yes/no\n",
    "                else:\n",
    "                    all_answers[key] = value\n",
    "    return all_answers\n",
    "\n",
    "def get_three_files(a_dir):\n",
    "    return [\n",
    "        a_dir + \"/factoid/predictions.json\",\n",
    "        a_dir + \"/list/predictions.json\",\n",
    "        a_dir + \"/yesno/predictions.json\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_xml(xml_file, dir_for_qa): \n",
    "    no_answers = 0\n",
    "    # get answers\n",
    "    qa_answers = get_answers(get_three_files(dir_for_qa))\n",
    "    # get ir and qu\n",
    "    df_cols = ['id','human_concepts','documents','full_abstracts','titles','type', 'exact_answer']\n",
    "    xtree = et.parse(xml_file)\n",
    "    xroot = xtree.getroot()\n",
    "    rows = []\n",
    "    for question in xroot: \n",
    "        id = question.attrib.get(\"id\")\n",
    "        ir = question.find(\"IR\")\n",
    "        qp = question.find(\"QP\")\n",
    "        concepts = [e.text for e in qp.findall(\"Entities\")]\n",
    "        qa_type = qp.find(\"Type\").text\n",
    "        titles =  [e.find(\"Title\").text for e in ir.findall(\"Result\")]\n",
    "        abstracts =  [e.find(\"Abstract\").text for e in ir.findall(\"Result\")]\n",
    "        pmids = [e.get(\"PMID\") for e in ir.findall(\"Result\")]\n",
    "        exact_answer = qa_answers[id] if id in qa_answers else None\n",
    "        if DEBUG and not exact_answer:\n",
    "            print(f\"id [{id}] has no answer\")\n",
    "            no_answers +=1\n",
    "        rows.append({\"id\":id,\"human_concepts\":concepts,\"documents\":pmids,\"full_abstracts\":abstracts,\"titles\":titles,\"type\":qa_type,'exact_answer':exact_answer})\n",
    "    out_df = pd.DataFrame(rows, columns=df_cols)\n",
    "    if DEBUG:\n",
    "        print(f\"[{no_answers}/{len(out_df)}] questions had answers\")\n",
    "    return out_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the golden answer dataframe\n",
    "golden_dataset_path = \"testing_datasets/augmented_concepts_abstracts_titles.json\"\n",
    "generated_qu = \"tmp/ir/output/bioasq_qa.xml\"\n",
    "with open(golden_dataset_path,'r') as f:\n",
    "    gold_data = json.loads(f.read())\n",
    "# load and flatten data\n",
    "gold_df = pd.json_normalize(gold_data,record_path=\"questions\")\n",
    "# get gold df\n",
    "gold_df['documents'] = gold_df['documents'].apply(get_pmid)\n",
    "# get generated df\n",
    "gen_df = parse_xml(generated_qu,'tmp/qa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes/No Evaluation\n",
      "Gold\n",
      " [881] Gold Yes/No Questions\n",
      " [704] Gold Yes Questions\n",
      " [177] Gold No Questions\n",
      "Generated\n",
      " [909] Generated Yes/No Questions\n",
      " [0] Generated Yes Questions\n",
      " [523] Generated No Questions\n",
      "yes f1 eval (881) (3243)\n",
      "no question [5a67b48cb750ff4455000010] had generated answer empty\n",
      "ytp: 0, yfp: 0, yfn: 427\n",
      "Yes f1 0, precicion 0, recall 0.0\n",
      "no f1 eval\n",
      "no question [5a67b48cb750ff4455000010] had generated answer empty\n",
      "ntp: 78, nfp: 427, yfn: 0\n",
      "No f1 0.26758147512864494, precicion 0.15445544554455445, recall 1.0\n",
      "(0, 0, 0.0, 0.26758147512864494, 0.15445544554455445, 1.0)\n"
     ]
    }
   ],
   "source": [
    "def print_yes_no_info(df, tag):\n",
    "    print(tag)\n",
    "    print(f\" [{len(df)}] {tag} Yes/No Questions\")\n",
    "    yes_df = df[df['exact_answer'] == 'yes']\n",
    "    no_df = df[df['exact_answer'] == 'no']\n",
    "    print(f\" [{len(yes_df)}] {tag} Yes Questions\")\n",
    "    print(f\" [{len(no_df)}] {tag} No Questions\")\n",
    "\n",
    "\n",
    "\"\"\" f1 Yes\n",
    "    tp is gen 'yes' | gold 'yes'\n",
    "    fp is gen 'yes' | gold 'no'\n",
    "    fn is gen 'no' |  gold 'yes'\n",
    "\n",
    "    f1 No\n",
    "    tp is gen 'no' | gold 'no'\n",
    "    fp is gen 'no' | gold 'yes'\n",
    "    fn is gen 'yes' |  gold 'no'\n",
    "\n",
    "    IGNORE if the predicted type is yes/no but gold type is different\n",
    "\"\"\"\n",
    "def do_yes_no_eval(gold_df,gen_df):\n",
    "    print(\"Yes/No Evaluation\")\n",
    "    yes_no_gold_df = gold_df[gold_df['type'] == 'yesno']\n",
    "    yes_no_gen_df = gen_df[gen_df['type'] == 'yesno']\n",
    "    # Gold stats\n",
    "    print_yes_no_info(yes_no_gold_df, \"Gold\")\n",
    "    # Gen Stats\n",
    "    print_yes_no_info(yes_no_gen_df, \"Generated\")\n",
    "\n",
    "    gold_answers = yes_no_gold_df.loc[:,['id','exact_answer']].copy()\n",
    "    gen_answers = gen_df.loc[:,['id','exact_answer']].copy() # grabbing all because there could be incorrect type guesses\n",
    "    \n",
    "    gold = gold_answers.to_dict(orient='list')\n",
    "    gen = gen_answers.to_dict(orient='list')\n",
    "    gen_ids = gen['id']\n",
    "    gen_ans = gen['exact_answer']\n",
    "    gold_ids = gold['id']\n",
    "    gold_ans = gold['exact_answer']\n",
    "\n",
    "    # YES \n",
    "    print(f\"yes f1 eval ({len(gold_ids)}) ({len(gen_ids)})\")\n",
    "    ytp = 0\n",
    "    yfp = 0\n",
    "    yfn = 0\n",
    "\n",
    "    for i in range (len(gold_ids)):\n",
    "        gold_val = gold_ans[i]\n",
    "        gen_val = gen_ans[gen_ids.index(gold_ids[i])]\n",
    "        if gen_val:\n",
    "            if gold_val == 'yes':\n",
    "                if gen_val =='yes':\n",
    "                    ytp += 1\n",
    "                elif gen_val =='no':\n",
    "                    yfn += 1\n",
    "                else:\n",
    "                    print(f\"yes question [{gold_ids[i]}] had generated answer {gen_val}\")\n",
    "            elif gold_val == 'no':\n",
    "                if gen_val == 'yes':\n",
    "                    yfp +=1\n",
    "                elif gen_val =='no':\n",
    "                    pass #handled by no f1\n",
    "                else:\n",
    "                    print(f\"no question [{gold_ids[i]}] had generated answer {gen_val}\")\n",
    "            else:\n",
    "                print(f\"GOLDEN answer to yes/no question [{gold_ids[i]}] was {gold_val}\")\n",
    "                \n",
    "        else: # not identified as yes/no question by generated\n",
    "            pass\n",
    "    #sanity check\n",
    "    print (f\"ytp: {ytp}, yfp: {yfp}, yfn: {yfn}\")\n",
    "    try:\n",
    "        yp = ytp/(ytp + yfp)\n",
    "    except:\n",
    "        yp = 0\n",
    "    try:\n",
    "        yr = ytp/(ytp + yfn)\n",
    "    except:\n",
    "        yr = 0\n",
    "    try:\n",
    "        yf1 = 2 * ((yp * yr)/(yp+yr))\n",
    "    except:\n",
    "        yf1 = 0\n",
    "    print (f'Yes f1 {yf1}, precision {yp}, recall {yr}')\n",
    "\n",
    "    # NO SIDE\n",
    "    print(\"no f1 eval\")\n",
    "    ntp = 0\n",
    "    nfp = 0\n",
    "    nfn = 0\n",
    "\n",
    "    for i in range (len(gold_ids)):\n",
    "        gold_val = gold_ans[i]\n",
    "        gen_val = gen_ans[gen_ids.index(gold_ids[i])]\n",
    "        if gen_val:\n",
    "            if gold_val == 'no':\n",
    "                if gen_val =='no':\n",
    "                    ntp += 1\n",
    "                elif gen_val =='yes':\n",
    "                    nfn += 1\n",
    "                else:\n",
    "                    print(f\"no question [{gold_ids[i]}] had generated answer {gen_val}\")\n",
    "            elif gold_val == 'yes':\n",
    "                if gen_val == 'no':\n",
    "                    nfp +=1\n",
    "                elif gen_val =='yes':\n",
    "                    pass #handled by no f1\n",
    "                else:\n",
    "                    print(f\"yes question [{gold_ids[i]}] had generated answer {gen_val}\")\n",
    "            else:\n",
    "                print(f\"GOLDEN answer to yes/no question [{gold_ids[i]}] was {gold_val}\")\n",
    "                \n",
    "        else: # not identified as yes/no question by generated\n",
    "            pass\n",
    "\n",
    "    #sanity check\n",
    "    print (f\"ntp: {ntp}, nfp: {nfp}, yfn: {nfn}\")\n",
    "    try:\n",
    "        np = ntp/(ntp + nfp)\n",
    "    except:\n",
    "        np = 0\n",
    "    try:\n",
    "        nr = ntp/(ntp + nfn)\n",
    "    except:\n",
    "        nr = 0\n",
    "    try:\n",
    "        nf1 = 2 * ((np * nr)/(np+nr))\n",
    "    except:\n",
    "        nf1 = 0\n",
    "    print (f'No f1 {nf1}, precision {np}, recall {nr}')\n",
    "\n",
    "    return yf1,yp,yr,nf1,np,nr\n",
    "\n",
    "yes_no_results = do_yes_no_eval(gold_df,gen_df)\n",
    "print(yes_no_results)\n",
    "\n",
    "\n",
    "# so if the gold_gen[id].exact_answer == gen[exact_answer], tp ++ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report for type evaluation step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     factoid       0.93      0.84      0.88       941\n",
      "        list       0.94      0.95      0.95       644\n",
      "     summary       0.84      0.91      0.87       777\n",
      "       yesno       0.97      1.00      0.98       881\n",
      "\n",
      "    accuracy                           0.92      3243\n",
      "   macro avg       0.92      0.92      0.92      3243\n",
      "weighted avg       0.92      0.92      0.92      3243\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get classification reports\n",
    "print(\"Report for type evaluation step\")\n",
    "type_report = classification_report(gold_df['type'].to_numpy(),gen_df['type'].to_numpy(),output_dict=DEBUG)\n",
    "print(type_report)\n",
    "\n",
    "# yes_no_results = do_yes_no_eval(gold_df,gen_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yesn_no_gold_df = gold_df[gold_df['type'] == 'yesno']\n",
    "# factoid_gold_df = gold_df[gold_df['type'] == 'factoid']\n",
    "# list_gold_df = gold_df[gold_df['type'] == 'list']\n",
    "\n",
    "# yesn_no_gen_df = gen_df[gen_df['type'] == 'yesno']\n",
    "# factoid_gen_df = gen_df[gen_df['type'] == 'factoid']\n",
    "# list_gen_df = gen_df[gen_df['type'] == 'list']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3243\n",
      "881\n"
     ]
    }
   ],
   "source": [
    "yes_no_gold_df = gold_df[gold_df['type'] == 'yesno']\n",
    "yes_no_gen_df = gen_df[gen_df['type'] == 'yesno']\n",
    "\n",
    "gold_answers = yes_no_gold_df.loc[:,['id','exact_answer']].copy()\n",
    "gen_answers = gen_df.loc[:,['id','exact_answer']].copy() \n",
    "\n",
    "gold = gold_answers.to_dict(orient='list')\n",
    "gen = gen_answers.to_dict(orient='list')\n",
    "\n",
    "gen_ids = gen['id']\n",
    "gen_ans = gen['exact_answer']\n",
    "\n",
    "gold_ids = gold['id']\n",
    "gold_ans = gold['exact_answer']\n",
    "\n",
    "print(len(gen_ids))\n",
    "print(len(gold_ids))\n",
    "\n",
    "# YES \n",
    "for gold_id in gold_ids:\n",
    "    print()\n",
    "\n",
    "# NO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 2\n",
    "c+=1\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "80370dbcfc3023cdbcb71d202b3d45870b2ec245031d0411f339cfb8d50c0055"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('3.7.9': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
